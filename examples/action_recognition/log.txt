I0406 18:07:58.331473 86685 caffe.cpp:367] min sub thread id:0 node name:ubuntu105
I0406 18:07:58.331473 86686 caffe.cpp:367] min sub thread id:0 node name:ubuntu105
I0406 18:07:58.345044 86686 caffe.cpp:374] FLAGS_gpu:1
I0406 18:07:58.345232 86685 caffe.cpp:374] FLAGS_gpu:0
I0406 18:07:58.714567 86685 caffe.cpp:139] Use GPU with device ID 0
I0406 18:07:58.714607 86685 caffe.cpp:147] Starting Optimization
I0406 18:07:58.714658 86685 solver.cpp:48] Initializing solver from parameters: 
test_iter: 80
test_interval: 100
base_lr: 0.001
display: 20
max_iter: 30000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
stepsize: 4000
snapshot: 1000
snapshot_prefix: "snapshots_rgb"
solver_mode: GPU
random_seed: 1701
net: "train_test_lstm.prototxt"
test_state {
  stage: "test-on-test"
}
test_initialization: false
average_loss: 1000
I0406 18:07:58.714701 86685 solver.cpp:86] Creating training net from net file: train_test_lstm.prototxt
I0406 18:07:58.715857 86685 net.cpp:287] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0406 18:07:58.716215 86685 net.cpp:42] Initializing net from parameters: 
name: "lstm_joints"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "VideoDataRNN"
  top: "data"
  top: "label"
  top: "clip_marker"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
    fix_crop: false
    multi_scale: true
    scale_ratios: 1
    scale_ratios: 0.875
    scale_ratios: 0.75
    is_flow: false
  }
  video_data_param {
    source: "/data2/action/rgb/list/train_video.txt"
    batch_size: 32
    shuffle: true
    new_height: 240
    new_width: 320
    modality: RGB
    root_folder: "/data2/action/rgb/"
    seq_length: 16
    stride_seq: 6
    eval_last: true
  }
}
layer {
  name: "conv1"
  type: "ConvolutionData"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn1"
  type: "BNData"
  bottom: "conv1"
  top: "conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1/bn"
  top: "conv1/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "ConvolutionData"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 5
    group: 2
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn2"
  type: "BNData"
  bottom: "conv2"
  top: "conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2/bn"
  top: "conv2/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "ConvolutionData"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn3"
  type: "BNData"
  bottom: "conv3"
  top: "conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3/bn"
  top: "conv3/bn"
}
layer {
  name: "conv4"
  type: "ConvolutionData"
  bottom: "conv3/bn"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn4"
  type: "BNData"
  bottom: "conv4"
  top: "conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4/bn"
  top: "conv4/bn"
}
layer {
  name: "conv5"
  type: "ConvolutionData"
  bottom: "conv4/bn"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn5"
  type: "BNData"
  bottom: "conv5"
  top: "conv5/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5/bn"
  top: "conv5/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5/bn"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reshape_data"
  type: "Reshape"
  bottom: "fc6"
  top: "data_reshape"
  reshape_param {
    shape {
      dim: 16
      dim: -1
      dim: 4096
    }
  }
}
layer {
  name: "lstm"
  type: "SLLSTM"
  bottom: "data_reshape"
  bottom: "clip_marker"
  top: "lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "lstm_drop"
  type: "Dropout"
  bottom: "lstm"
  top: "lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "data_reshape"
  top: "inv_lstm_input"
}
layer {
  name: "invlstm"
  type: "SLLSTM"
  bottom: "inv_lstm_input"
  bottom: "clip_marker"
  top: "inv_lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "inv_lstm_drop"
  type: "Dropout"
  bottom: "inv_lstm"
  top: "inv_lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "inv_lstm_drop"
  top: "inv_lstm_output"
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "lstm_drop"
  bottom: "inv_lstm_output"
  top: "lstm_concat"
  concat_param {
    axis: 2
  }
}
layer {
  name: "fc8_final"
  type: "InnerProduct"
  bottom: "lstm_concat"
  top: "fc8_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 12
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_final"
  bottom: "label"
  top: "loss"
  loss_param {
    ignore_label: -1
  }
  softmax_param {
    axis: 2
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy"
  accuracy_param {
    axis: 2
    ignore_label: -1
  }
}
layer {
  name: "accuracy_top2"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy_top2"
  accuracy_param {
    top_k: 2
    axis: 2
    ignore_label: -1
  }
}
I0406 18:07:58.716547 86685 layer_factory.hpp:74] Creating layer data
I0406 18:07:58.716595 86685 net.cpp:90] Creating Layer data
I0406 18:07:58.716603 86685 net.cpp:368] data -> data
I0406 18:07:58.716651 86685 net.cpp:368] data -> label
I0406 18:07:58.716663 86685 net.cpp:368] data -> clip_marker
I0406 18:07:58.716670 86685 net.cpp:120] Setting up data
I0406 18:07:58.716723 86685 video_data_rnn_layer.cpp:42] Opening file: /data2/action/rgb/list/train_video.txt
I0406 18:07:58.716958 86685 video_data_rnn_layer.cpp:58] A total of 180 videos.
I0406 18:07:58.718353 86686 caffe.cpp:139] Use GPU with device ID 1
I0406 18:07:58.718384 86686 caffe.cpp:147] Starting Optimization
I0406 18:07:58.718435 86686 solver.cpp:48] Initializing solver from parameters: 
test_iter: 80
test_interval: 100
base_lr: 0.001
display: 20
max_iter: 30000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
stepsize: 4000
snapshot: 1000
snapshot_prefix: "snapshots_rgb"
solver_mode: GPU
random_seed: 1701
net: "train_test_lstm.prototxt"
test_state {
  stage: "test-on-test"
}
test_initialization: false
average_loss: 1000
I0406 18:07:58.718457 86686 solver.cpp:86] Creating training net from net file: train_test_lstm.prototxt
I0406 18:07:58.719553 86686 net.cpp:287] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0406 18:07:58.719882 86686 net.cpp:42] Initializing net from parameters: 
name: "lstm_joints"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "VideoDataRNN"
  top: "data"
  top: "label"
  top: "clip_marker"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
    fix_crop: false
    multi_scale: true
    scale_ratios: 1
    scale_ratios: 0.875
    scale_ratios: 0.75
    is_flow: false
  }
  video_data_param {
    source: "/data2/action/rgb/list/train_video.txt"
    batch_size: 32
    shuffle: true
    new_height: 240
    new_width: 320
    modality: RGB
    root_folder: "/data2/action/rgb/"
    seq_length: 16
    stride_seq: 6
    eval_last: true
  }
}
layer {
  name: "conv1"
  type: "ConvolutionData"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn1"
  type: "BNData"
  bottom: "conv1"
  top: "conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1/bn"
  top: "conv1/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "ConvolutionData"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 5
    group: 2
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn2"
  type: "BNData"
  bottom: "conv2"
  top: "conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2/bn"
  top: "conv2/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "ConvolutionData"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn3"
  type: "BNData"
  bottom: "conv3"
  top: "conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3/bn"
  top: "conv3/bn"
}
layer {
  name: "conv4"
  type: "ConvolutionData"
  bottom: "conv3/bn"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn4"
  type: "BNData"
  bottom: "conv4"
  top: "conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4/bn"
  top: "conv4/bn"
}
layer {
  name: "conv5"
  type: "ConvolutionData"
  bottom: "conv4/bn"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn5"
  type: "BNData"
  bottom: "conv5"
  top: "conv5/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5/bn"
  top: "conv5/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5/bn"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reshape_data"
  type: "Reshape"
  bottom: "fc6"
  top: "data_reshape"
  reshape_param {
    shape {
      dim: 16
      dim: -1
      dim: 4096
    }
  }
}
layer {
  name: "lstm"
  type: "SLLSTM"
  bottom: "data_reshape"
  bottom: "clip_marker"
  top: "lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "lstm_drop"
  type: "Dropout"
  bottom: "lstm"
  top: "lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "data_reshape"
  top: "inv_lstm_input"
}
layer {
  name: "invlstm"
  type: "SLLSTM"
  bottom: "inv_lstm_input"
  bottom: "clip_marker"
  top: "inv_lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "inv_lstm_drop"
  type: "Dropout"
  bottom: "inv_lstm"
  top: "inv_lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "inv_lstm_drop"
  top: "inv_lstm_output"
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "lstm_drop"
  bottom: "inv_lstm_output"
  top: "lstm_concat"
  concat_param {
    axis: 2
  }
}
layer {
  name: "fc8_final"
  type: "InnerProduct"
  bottom: "lstm_concat"
  top: "fc8_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 12
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_final"
  bottom: "label"
  top: "loss"
  loss_param {
    ignore_label: -1
  }
  softmax_param {
    axis: 2
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy"
  accuracy_param {
    axis: 2
    ignore_label: -1
  }
}
layer {
  name: "accuracy_top2"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy_top2"
  accuracy_param {
    top_k: 2
    axis: 2
    ignore_label: -1
  }
}
I0406 18:07:58.720185 86686 layer_factory.hpp:74] Creating layer data
I0406 18:07:58.720217 86686 net.cpp:90] Creating Layer data
I0406 18:07:58.720237 86686 net.cpp:368] data -> data
I0406 18:07:58.720257 86686 net.cpp:368] data -> label
I0406 18:07:58.720273 86686 net.cpp:368] data -> clip_marker
I0406 18:07:58.720283 86686 net.cpp:120] Setting up data
I0406 18:07:58.720314 86686 video_data_rnn_layer.cpp:42] Opening file: /data2/action/rgb/list/train_video.txt
I0406 18:07:58.720469 86686 video_data_rnn_layer.cpp:58] A total of 180 videos.
I0406 18:07:58.762903 86685 net.cpp:127] Top shape: 256 3 227 227 (39574272)
I0406 18:07:58.762950 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.762960 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.762971 86685 layer_factory.hpp:74] Creating layer label_data_1_split
I0406 18:07:58.762989 86685 net.cpp:90] Creating Layer label_data_1_split
I0406 18:07:58.763000 86685 net.cpp:410] label_data_1_split <- label
I0406 18:07:58.763031 86685 net.cpp:368] label_data_1_split -> label_data_1_split_0
I0406 18:07:58.763051 86685 net.cpp:368] label_data_1_split -> label_data_1_split_1
I0406 18:07:58.763065 86685 net.cpp:368] label_data_1_split -> label_data_1_split_2
I0406 18:07:58.763106 86685 net.cpp:120] Setting up label_data_1_split
I0406 18:07:58.763139 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763164 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763180 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763188 86685 layer_factory.hpp:74] Creating layer clip_marker_data_2_split
I0406 18:07:58.763206 86685 net.cpp:90] Creating Layer clip_marker_data_2_split
I0406 18:07:58.763216 86685 net.cpp:410] clip_marker_data_2_split <- clip_marker
I0406 18:07:58.763229 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_0
I0406 18:07:58.763247 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_1
I0406 18:07:58.763262 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_2
I0406 18:07:58.763290 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_3
I0406 18:07:58.763303 86685 net.cpp:120] Setting up clip_marker_data_2_split
I0406 18:07:58.763348 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763361 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763375 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763386 86685 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.763397 86685 layer_factory.hpp:74] Creating layer conv1
I0406 18:07:58.763417 86685 net.cpp:90] Creating Layer conv1
I0406 18:07:58.763427 86685 net.cpp:410] conv1 <- data
I0406 18:07:58.763442 86685 net.cpp:368] conv1 -> conv1
I0406 18:07:58.763461 86685 net.cpp:120] Setting up conv1
I0406 18:07:58.766064 86686 net.cpp:127] Top shape: 256 3 227 227 (39574272)
I0406 18:07:58.766091 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766101 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766111 86686 layer_factory.hpp:74] Creating layer label_data_1_split
I0406 18:07:58.766139 86686 net.cpp:90] Creating Layer label_data_1_split
I0406 18:07:58.766150 86686 net.cpp:410] label_data_1_split <- label
I0406 18:07:58.766170 86686 net.cpp:368] label_data_1_split -> label_data_1_split_0
I0406 18:07:58.766186 86686 net.cpp:368] label_data_1_split -> label_data_1_split_1
I0406 18:07:58.766204 86686 net.cpp:368] label_data_1_split -> label_data_1_split_2
I0406 18:07:58.766219 86686 net.cpp:120] Setting up label_data_1_split
I0406 18:07:58.766237 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766250 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766263 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766273 86686 layer_factory.hpp:74] Creating layer clip_marker_data_2_split
I0406 18:07:58.766286 86686 net.cpp:90] Creating Layer clip_marker_data_2_split
I0406 18:07:58.766297 86686 net.cpp:410] clip_marker_data_2_split <- clip_marker
I0406 18:07:58.766310 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_0
I0406 18:07:58.766326 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_1
I0406 18:07:58.766342 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_2
I0406 18:07:58.766360 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_3
I0406 18:07:58.766372 86686 net.cpp:120] Setting up clip_marker_data_2_split
I0406 18:07:58.766413 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766427 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766440 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766453 86686 net.cpp:127] Top shape: 16 16 1 (256)
I0406 18:07:58.766472 86686 layer_factory.hpp:74] Creating layer conv1
I0406 18:07:58.766494 86686 net.cpp:90] Creating Layer conv1
I0406 18:07:58.766504 86686 net.cpp:410] conv1 <- data
I0406 18:07:58.766518 86686 net.cpp:368] conv1 -> conv1
I0406 18:07:58.766537 86686 net.cpp:120] Setting up conv1
I0406 18:07:59.764317 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 221796
I0406 18:07:59.764477 86685 net.cpp:127] Top shape: 256 96 111 111 (302800896)
I0406 18:07:59.764516 86685 layer_factory.hpp:74] Creating layer bn1
I0406 18:07:59.764542 86685 net.cpp:90] Creating Layer bn1
I0406 18:07:59.764567 86685 net.cpp:410] bn1 <- conv1
I0406 18:07:59.764576 86685 net.cpp:368] bn1 -> conv1/bn
I0406 18:07:59.764600 86685 net.cpp:120] Setting up bn1
I0406 18:07:59.764653 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.764703 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.764722 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.764730 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.764737 86685 net.cpp:127] Top shape: 256 96 111 111 (302800896)
I0406 18:07:59.764747 86685 layer_factory.hpp:74] Creating layer relu1
I0406 18:07:59.764765 86685 net.cpp:90] Creating Layer relu1
I0406 18:07:59.764770 86685 net.cpp:410] relu1 <- conv1/bn
I0406 18:07:59.764775 86685 net.cpp:357] relu1 -> conv1/bn (in-place)
I0406 18:07:59.764781 86685 net.cpp:120] Setting up relu1
I0406 18:07:59.765077 86685 net.cpp:127] Top shape: 256 96 111 111 (302800896)
I0406 18:07:59.765089 86685 layer_factory.hpp:74] Creating layer pool1
I0406 18:07:59.765110 86685 net.cpp:90] Creating Layer pool1
I0406 18:07:59.765115 86685 net.cpp:410] pool1 <- conv1/bn
I0406 18:07:59.765120 86685 net.cpp:368] pool1 -> pool1
I0406 18:07:59.765126 86685 net.cpp:120] Setting up pool1
I0406 18:07:59.765324 86685 net.cpp:127] Top shape: 256 96 55 55 (74342400)
I0406 18:07:59.765332 86685 layer_factory.hpp:74] Creating layer norm1
I0406 18:07:59.765352 86685 net.cpp:90] Creating Layer norm1
I0406 18:07:59.765357 86685 net.cpp:410] norm1 <- pool1
I0406 18:07:59.765372 86685 net.cpp:368] norm1 -> norm1
I0406 18:07:59.765379 86685 net.cpp:120] Setting up norm1
I0406 18:07:59.765385 86685 net.cpp:127] Top shape: 256 96 55 55 (74342400)
I0406 18:07:59.765401 86685 layer_factory.hpp:74] Creating layer conv2
I0406 18:07:59.765411 86685 net.cpp:90] Creating Layer conv2
I0406 18:07:59.765424 86685 net.cpp:410] conv2 <- norm1
I0406 18:07:59.765429 86685 net.cpp:368] conv2 -> conv2
I0406 18:07:59.765436 86685 net.cpp:120] Setting up conv2
I0406 18:07:59.850776 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 221796
I0406 18:07:59.850966 86686 net.cpp:127] Top shape: 256 96 111 111 (302800896)
I0406 18:07:59.851016 86686 layer_factory.hpp:74] Creating layer bn1
I0406 18:07:59.851033 86686 net.cpp:90] Creating Layer bn1
I0406 18:07:59.851058 86686 net.cpp:410] bn1 <- conv1
I0406 18:07:59.851078 86686 net.cpp:368] bn1 -> conv1/bn
I0406 18:07:59.851095 86686 net.cpp:120] Setting up bn1
I0406 18:07:59.851187 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.851222 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.851240 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.851246 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.851254 86686 net.cpp:127] Top shape: 256 96 111 111 (302800896)
I0406 18:07:59.851276 86686 layer_factory.hpp:74] Creating layer relu1
I0406 18:07:59.851289 86686 net.cpp:90] Creating Layer relu1
I0406 18:07:59.851294 86686 net.cpp:410] relu1 <- conv1/bn
I0406 18:07:59.851299 86686 net.cpp:357] relu1 -> conv1/bn (in-place)
I0406 18:07:59.851305 86686 net.cpp:120] Setting up relu1
I0406 18:07:59.851609 86686 net.cpp:127] Top shape: 256 96 111 111 (302800896)
I0406 18:07:59.851619 86686 layer_factory.hpp:74] Creating layer pool1
I0406 18:07:59.851641 86686 net.cpp:90] Creating Layer pool1
I0406 18:07:59.851645 86686 net.cpp:410] pool1 <- conv1/bn
I0406 18:07:59.851651 86686 net.cpp:368] pool1 -> pool1
I0406 18:07:59.851658 86686 net.cpp:120] Setting up pool1
I0406 18:07:59.851866 86686 net.cpp:127] Top shape: 256 96 55 55 (74342400)
I0406 18:07:59.851874 86686 layer_factory.hpp:74] Creating layer norm1
I0406 18:07:59.851893 86686 net.cpp:90] Creating Layer norm1
I0406 18:07:59.851897 86686 net.cpp:410] norm1 <- pool1
I0406 18:07:59.851902 86686 net.cpp:368] norm1 -> norm1
I0406 18:07:59.851910 86686 net.cpp:120] Setting up norm1
I0406 18:07:59.851917 86686 net.cpp:127] Top shape: 256 96 55 55 (74342400)
I0406 18:07:59.851920 86686 layer_factory.hpp:74] Creating layer conv2
I0406 18:07:59.851940 86686 net.cpp:90] Creating Layer conv2
I0406 18:07:59.851945 86686 net.cpp:410] conv2 <- norm1
I0406 18:07:59.851951 86686 net.cpp:368] conv2 -> conv2
I0406 18:07:59.851958 86686 net.cpp:120] Setting up conv2
I0406 18:07:59.854555 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 37392
I0406 18:07:59.854591 86686 net.cpp:127] Top shape: 256 384 26 26 (66453504)
I0406 18:07:59.854599 86686 layer_factory.hpp:74] Creating layer bn2
I0406 18:07:59.854607 86686 net.cpp:90] Creating Layer bn2
I0406 18:07:59.854611 86686 net.cpp:410] bn2 <- conv2
I0406 18:07:59.854629 86686 net.cpp:368] bn2 -> conv2/bn
I0406 18:07:59.854650 86686 net.cpp:120] Setting up bn2
I0406 18:07:59.854837 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 37392
I0406 18:07:59.854887 86685 net.cpp:127] Top shape: 256 384 26 26 (66453504)
I0406 18:07:59.854915 86685 layer_factory.hpp:74] Creating layer bn2
I0406 18:07:59.854931 86685 net.cpp:90] Creating Layer bn2
I0406 18:07:59.854939 86685 net.cpp:410] bn2 <- conv2
I0406 18:07:59.854948 86685 net.cpp:368] bn2 -> conv2/bn
I0406 18:07:59.854956 86685 net.cpp:120] Setting up bn2
I0406 18:07:59.854987 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.854987 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.855001 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.855012 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.855021 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.855021 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.855042 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.855042 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.855063 86686 net.cpp:127] Top shape: 256 384 26 26 (66453504)
I0406 18:07:59.855063 86685 net.cpp:127] Top shape: 256 384 26 26 (66453504)
I0406 18:07:59.855084 86686 layer_factory.hpp:74] Creating layer relu2
I0406 18:07:59.855085 86685 layer_factory.hpp:74] Creating layer relu2
I0406 18:07:59.855090 86686 net.cpp:90] Creating Layer relu2
I0406 18:07:59.855104 86685 net.cpp:90] Creating Layer relu2
I0406 18:07:59.855106 86686 net.cpp:410] relu2 <- conv2/bn
I0406 18:07:59.855111 86686 net.cpp:357] relu2 -> conv2/bn (in-place)
I0406 18:07:59.855108 86685 net.cpp:410] relu2 <- conv2/bn
I0406 18:07:59.855114 86685 net.cpp:357] relu2 -> conv2/bn (in-place)
I0406 18:07:59.855118 86686 net.cpp:120] Setting up relu2
I0406 18:07:59.855120 86685 net.cpp:120] Setting up relu2
I0406 18:07:59.855321 86686 net.cpp:127] Top shape: 256 384 26 26 (66453504)
I0406 18:07:59.855329 86686 layer_factory.hpp:74] Creating layer pool2
I0406 18:07:59.855329 86685 net.cpp:127] Top shape: 256 384 26 26 (66453504)
I0406 18:07:59.855347 86685 layer_factory.hpp:74] Creating layer pool2
I0406 18:07:59.855361 86686 net.cpp:90] Creating Layer pool2
I0406 18:07:59.855366 86686 net.cpp:410] pool2 <- conv2/bn
I0406 18:07:59.855370 86685 net.cpp:90] Creating Layer pool2
I0406 18:07:59.855371 86686 net.cpp:368] pool2 -> pool2
I0406 18:07:59.855375 86685 net.cpp:410] pool2 <- conv2/bn
I0406 18:07:59.855391 86686 net.cpp:120] Setting up pool2
I0406 18:07:59.855392 86685 net.cpp:368] pool2 -> pool2
I0406 18:07:59.855401 86685 net.cpp:120] Setting up pool2
I0406 18:07:59.855851 86686 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.855876 86686 layer_factory.hpp:74] Creating layer norm2
I0406 18:07:59.855885 86686 net.cpp:90] Creating Layer norm2
I0406 18:07:59.855888 86686 net.cpp:410] norm2 <- pool2
I0406 18:07:59.855895 86686 net.cpp:368] norm2 -> norm2
I0406 18:07:59.855901 86686 net.cpp:120] Setting up norm2
I0406 18:07:59.855921 86686 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.855926 86686 layer_factory.hpp:74] Creating layer conv3
I0406 18:07:59.855932 86686 net.cpp:90] Creating Layer conv3
I0406 18:07:59.855937 86686 net.cpp:410] conv3 <- norm2
I0406 18:07:59.855942 86686 net.cpp:368] conv3 -> conv3
I0406 18:07:59.855943 86685 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.855949 86686 net.cpp:120] Setting up conv3
I0406 18:07:59.855968 86685 layer_factory.hpp:74] Creating layer norm2
I0406 18:07:59.855978 86685 net.cpp:90] Creating Layer norm2
I0406 18:07:59.855981 86685 net.cpp:410] norm2 <- pool2
I0406 18:07:59.855999 86685 net.cpp:368] norm2 -> norm2
I0406 18:07:59.856019 86685 net.cpp:120] Setting up norm2
I0406 18:07:59.856026 86685 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.856042 86685 layer_factory.hpp:74] Creating layer conv3
I0406 18:07:59.856052 86685 net.cpp:90] Creating Layer conv3
I0406 18:07:59.856056 86685 net.cpp:410] conv3 <- norm2
I0406 18:07:59.856062 86685 net.cpp:368] conv3 -> conv3
I0406 18:07:59.856079 86685 net.cpp:120] Setting up conv3
I0406 18:07:59.907989 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 9588
I0406 18:07:59.908093 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 9588
I0406 18:07:59.908169 86686 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.908193 86686 layer_factory.hpp:74] Creating layer bn3
I0406 18:07:59.908205 86686 net.cpp:90] Creating Layer bn3
I0406 18:07:59.908210 86686 net.cpp:410] bn3 <- conv3
I0406 18:07:59.908216 86686 net.cpp:368] bn3 -> conv3/bn
I0406 18:07:59.908226 86686 net.cpp:120] Setting up bn3
I0406 18:07:59.908277 86685 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.908293 86685 layer_factory.hpp:74] Creating layer bn3
I0406 18:07:59.908303 86685 net.cpp:90] Creating Layer bn3
I0406 18:07:59.908308 86685 net.cpp:410] bn3 <- conv3
I0406 18:07:59.908329 86685 net.cpp:368] bn3 -> conv3/bn
I0406 18:07:59.908337 86685 net.cpp:120] Setting up bn3
I0406 18:07:59.908385 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.908385 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.908402 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.908401 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.908423 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.908423 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.908449 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.908449 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.908457 86685 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.908457 86686 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.908478 86685 layer_factory.hpp:74] Creating layer relu3
I0406 18:07:59.908479 86686 layer_factory.hpp:74] Creating layer relu3
I0406 18:07:59.908484 86685 net.cpp:90] Creating Layer relu3
I0406 18:07:59.908500 86685 net.cpp:410] relu3 <- conv3/bn
I0406 18:07:59.908498 86686 net.cpp:90] Creating Layer relu3
I0406 18:07:59.908501 86686 net.cpp:410] relu3 <- conv3/bn
I0406 18:07:59.908506 86685 net.cpp:357] relu3 -> conv3/bn (in-place)
I0406 18:07:59.908507 86686 net.cpp:357] relu3 -> conv3/bn (in-place)
I0406 18:07:59.908514 86686 net.cpp:120] Setting up relu3
I0406 18:07:59.908514 86685 net.cpp:120] Setting up relu3
I0406 18:07:59.908743 86685 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.908752 86685 layer_factory.hpp:74] Creating layer conv4
I0406 18:07:59.908763 86685 net.cpp:90] Creating Layer conv4
I0406 18:07:59.908767 86685 net.cpp:410] conv4 <- conv3/bn
I0406 18:07:59.908752 86686 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.908759 86686 layer_factory.hpp:74] Creating layer conv4
I0406 18:07:59.908769 86686 net.cpp:90] Creating Layer conv4
I0406 18:07:59.908773 86686 net.cpp:410] conv4 <- conv3/bn
I0406 18:07:59.908774 86685 net.cpp:368] conv4 -> conv4
I0406 18:07:59.908793 86685 net.cpp:120] Setting up conv4
I0406 18:07:59.908792 86686 net.cpp:368] conv4 -> conv4
I0406 18:07:59.908799 86686 net.cpp:120] Setting up conv4
I0406 18:07:59.944347 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:07:59.944391 86685 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.944401 86685 layer_factory.hpp:74] Creating layer bn4
I0406 18:07:59.944411 86685 net.cpp:90] Creating Layer bn4
I0406 18:07:59.944434 86685 net.cpp:410] bn4 <- conv4
I0406 18:07:59.944442 86685 net.cpp:368] bn4 -> conv4/bn
I0406 18:07:59.944450 86685 net.cpp:120] Setting up bn4
I0406 18:07:59.944488 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.944504 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.944517 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.944511 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:07:59.944526 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.944533 86685 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.944540 86686 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.944542 86685 layer_factory.hpp:74] Creating layer relu4
I0406 18:07:59.944561 86685 net.cpp:90] Creating Layer relu4
I0406 18:07:59.944563 86686 layer_factory.hpp:74] Creating layer bn4
I0406 18:07:59.944566 86685 net.cpp:410] relu4 <- conv4/bn
I0406 18:07:59.944571 86685 net.cpp:357] relu4 -> conv4/bn (in-place)
I0406 18:07:59.944574 86686 net.cpp:90] Creating Layer bn4
I0406 18:07:59.944591 86686 net.cpp:410] bn4 <- conv4
I0406 18:07:59.944588 86685 net.cpp:120] Setting up relu4
I0406 18:07:59.944598 86686 net.cpp:368] bn4 -> conv4/bn
I0406 18:07:59.944619 86686 net.cpp:120] Setting up bn4
I0406 18:07:59.944658 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.944674 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.944684 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.944695 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.944702 86686 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.944712 86686 layer_factory.hpp:74] Creating layer relu4
I0406 18:07:59.944720 86686 net.cpp:90] Creating Layer relu4
I0406 18:07:59.944723 86686 net.cpp:410] relu4 <- conv4/bn
I0406 18:07:59.944730 86686 net.cpp:357] relu4 -> conv4/bn (in-place)
I0406 18:07:59.944736 86686 net.cpp:120] Setting up relu4
I0406 18:07:59.944771 86685 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.944779 86685 layer_factory.hpp:74] Creating layer conv5
I0406 18:07:59.944788 86685 net.cpp:90] Creating Layer conv5
I0406 18:07:59.944792 86685 net.cpp:410] conv5 <- conv4/bn
I0406 18:07:59.944799 86685 net.cpp:368] conv5 -> conv5
I0406 18:07:59.944807 86685 net.cpp:120] Setting up conv5
I0406 18:07:59.944905 86686 net.cpp:127] Top shape: 256 512 13 13 (22151168)
I0406 18:07:59.944913 86686 layer_factory.hpp:74] Creating layer conv5
I0406 18:07:59.944934 86686 net.cpp:90] Creating Layer conv5
I0406 18:07:59.944938 86686 net.cpp:410] conv5 <- conv4/bn
I0406 18:07:59.944947 86686 net.cpp:368] conv5 -> conv5
I0406 18:07:59.944954 86686 net.cpp:120] Setting up conv5
I0406 18:07:59.973667 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:07:59.973721 86685 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.973731 86685 layer_factory.hpp:74] Creating layer bn5
I0406 18:07:59.973739 86685 net.cpp:90] Creating Layer bn5
I0406 18:07:59.973757 86685 net.cpp:410] bn5 <- conv5
I0406 18:07:59.973764 86685 net.cpp:368] bn5 -> conv5/bn
I0406 18:07:59.973773 86685 net.cpp:120] Setting up bn5
I0406 18:07:59.973788 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.973824 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.973840 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.973853 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.973860 86685 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.973870 86685 layer_factory.hpp:74] Creating layer relu5
I0406 18:07:59.973876 86685 net.cpp:90] Creating Layer relu5
I0406 18:07:59.973881 86685 net.cpp:410] relu5 <- conv5/bn
I0406 18:07:59.973886 86685 net.cpp:357] relu5 -> conv5/bn (in-place)
I0406 18:07:59.973867 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:07:59.973893 86685 net.cpp:120] Setting up relu5
I0406 18:07:59.973909 86686 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.973922 86686 layer_factory.hpp:74] Creating layer bn5
I0406 18:07:59.973933 86686 net.cpp:90] Creating Layer bn5
I0406 18:07:59.973942 86686 net.cpp:410] bn5 <- conv5
I0406 18:07:59.973950 86686 net.cpp:368] bn5 -> conv5/bn
I0406 18:07:59.973959 86686 net.cpp:120] Setting up bn5
I0406 18:07:59.973980 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:07:59.973994 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:07:59.974004 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:07:59.974022 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:07:59.974035 86686 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.974043 86686 layer_factory.hpp:74] Creating layer relu5
I0406 18:07:59.974048 86686 net.cpp:90] Creating Layer relu5
I0406 18:07:59.974052 86686 net.cpp:410] relu5 <- conv5/bn
I0406 18:07:59.974057 86686 net.cpp:357] relu5 -> conv5/bn (in-place)
I0406 18:07:59.974063 86686 net.cpp:120] Setting up relu5
I0406 18:07:59.974073 86685 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.974083 86685 layer_factory.hpp:74] Creating layer pool5
I0406 18:07:59.974092 86685 net.cpp:90] Creating Layer pool5
I0406 18:07:59.974095 86685 net.cpp:410] pool5 <- conv5/bn
I0406 18:07:59.974102 86685 net.cpp:368] pool5 -> pool5
I0406 18:07:59.974108 86685 net.cpp:120] Setting up pool5
I0406 18:07:59.974242 86686 net.cpp:127] Top shape: 256 384 13 13 (16613376)
I0406 18:07:59.974249 86686 layer_factory.hpp:74] Creating layer pool5
I0406 18:07:59.974270 86686 net.cpp:90] Creating Layer pool5
I0406 18:07:59.974275 86686 net.cpp:410] pool5 <- conv5/bn
I0406 18:07:59.974280 86686 net.cpp:368] pool5 -> pool5
I0406 18:07:59.974287 86686 net.cpp:120] Setting up pool5
I0406 18:07:59.974407 86685 net.cpp:127] Top shape: 256 384 6 6 (3538944)
I0406 18:07:59.974419 86685 layer_factory.hpp:74] Creating layer fc6
I0406 18:07:59.974441 86685 net.cpp:90] Creating Layer fc6
I0406 18:07:59.974444 86685 net.cpp:410] fc6 <- pool5
I0406 18:07:59.974452 86685 net.cpp:368] fc6 -> fc6
I0406 18:07:59.974462 86685 net.cpp:120] Setting up fc6
I0406 18:07:59.974656 86686 net.cpp:127] Top shape: 256 384 6 6 (3538944)
I0406 18:07:59.974668 86686 layer_factory.hpp:74] Creating layer fc6
I0406 18:07:59.974688 86686 net.cpp:90] Creating Layer fc6
I0406 18:07:59.974691 86686 net.cpp:410] fc6 <- pool5
I0406 18:07:59.974700 86686 net.cpp:368] fc6 -> fc6
I0406 18:07:59.974709 86686 net.cpp:120] Setting up fc6
I0406 18:08:01.551865 86686 net.cpp:127] Top shape: 256 4096 (1048576)
I0406 18:08:01.551918 86686 layer_factory.hpp:74] Creating layer relu6
I0406 18:08:01.551935 86686 net.cpp:90] Creating Layer relu6
I0406 18:08:01.551945 86686 net.cpp:410] relu6 <- fc6
I0406 18:08:01.551954 86686 net.cpp:357] relu6 -> fc6 (in-place)
I0406 18:08:01.551962 86686 net.cpp:120] Setting up relu6
I0406 18:08:01.552534 86686 net.cpp:127] Top shape: 256 4096 (1048576)
I0406 18:08:01.552558 86686 layer_factory.hpp:74] Creating layer drop6
I0406 18:08:01.552582 86686 net.cpp:90] Creating Layer drop6
I0406 18:08:01.552585 86686 net.cpp:410] drop6 <- fc6
I0406 18:08:01.552590 86686 net.cpp:357] drop6 -> fc6 (in-place)
I0406 18:08:01.552614 86686 net.cpp:120] Setting up drop6
I0406 18:08:01.552626 86686 net.cpp:127] Top shape: 256 4096 (1048576)
I0406 18:08:01.552630 86686 layer_factory.hpp:74] Creating layer reshape_data
I0406 18:08:01.552657 86686 net.cpp:90] Creating Layer reshape_data
I0406 18:08:01.552662 86686 net.cpp:410] reshape_data <- fc6
I0406 18:08:01.552681 86686 net.cpp:368] reshape_data -> data_reshape
I0406 18:08:01.552691 86686 net.cpp:120] Setting up reshape_data
I0406 18:08:01.552700 86686 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.552721 86686 layer_factory.hpp:74] Creating layer data_reshape_reshape_data_0_split
I0406 18:08:01.552726 86686 net.cpp:90] Creating Layer data_reshape_reshape_data_0_split
I0406 18:08:01.552731 86686 net.cpp:410] data_reshape_reshape_data_0_split <- data_reshape
I0406 18:08:01.552748 86686 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_0
I0406 18:08:01.552755 86686 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_1
I0406 18:08:01.552774 86686 net.cpp:120] Setting up data_reshape_reshape_data_0_split
I0406 18:08:01.552780 86686 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.552785 86686 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.552788 86686 layer_factory.hpp:74] Creating layer lstm
I0406 18:08:01.552801 86686 net.cpp:90] Creating Layer lstm
I0406 18:08:01.552808 86686 net.cpp:410] lstm <- data_reshape_reshape_data_0_split_0
I0406 18:08:01.552824 86686 net.cpp:410] lstm <- clip_marker_data_2_split_0
I0406 18:08:01.552834 86686 net.cpp:368] lstm -> lstm
I0406 18:08:01.552860 86686 net.cpp:120] Setting up lstm
I0406 18:08:01.552876 86686 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.552893 86686 layer_factory.hpp:74] Creating layer lstm_drop
I0406 18:08:01.552901 86686 net.cpp:90] Creating Layer lstm_drop
I0406 18:08:01.552904 86686 net.cpp:410] lstm_drop <- lstm
I0406 18:08:01.552911 86686 net.cpp:368] lstm_drop -> lstm_drop
I0406 18:08:01.552917 86686 net.cpp:120] Setting up lstm_drop
I0406 18:08:01.552922 86686 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.552927 86686 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:01.552932 86686 net.cpp:90] Creating Layer reverse
I0406 18:08:01.552935 86686 net.cpp:410] reverse <- clip_marker_data_2_split_1
I0406 18:08:01.552940 86686 net.cpp:410] reverse <- data_reshape_reshape_data_0_split_1
I0406 18:08:01.552947 86686 net.cpp:368] reverse -> inv_lstm_input
I0406 18:08:01.552953 86686 net.cpp:120] Setting up reverse
I0406 18:08:01.552958 86686 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.552960 86686 layer_factory.hpp:74] Creating layer invlstm
I0406 18:08:01.552969 86686 net.cpp:90] Creating Layer invlstm
I0406 18:08:01.552974 86686 net.cpp:410] invlstm <- inv_lstm_input
I0406 18:08:01.552978 86686 net.cpp:410] invlstm <- clip_marker_data_2_split_2
I0406 18:08:01.552983 86686 net.cpp:368] invlstm -> inv_lstm
I0406 18:08:01.552994 86686 net.cpp:120] Setting up invlstm
I0406 18:08:01.553005 86686 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.553014 86686 layer_factory.hpp:74] Creating layer inv_lstm_drop
I0406 18:08:01.553023 86686 net.cpp:90] Creating Layer inv_lstm_drop
I0406 18:08:01.553026 86686 net.cpp:410] inv_lstm_drop <- inv_lstm
I0406 18:08:01.553035 86686 net.cpp:368] inv_lstm_drop -> inv_lstm_drop
I0406 18:08:01.553040 86686 net.cpp:120] Setting up inv_lstm_drop
I0406 18:08:01.553046 86686 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.553050 86686 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:01.553056 86686 net.cpp:90] Creating Layer reverse
I0406 18:08:01.553059 86686 net.cpp:410] reverse <- clip_marker_data_2_split_3
I0406 18:08:01.553063 86686 net.cpp:410] reverse <- inv_lstm_drop
I0406 18:08:01.553068 86686 net.cpp:368] reverse -> inv_lstm_output
I0406 18:08:01.553073 86686 net.cpp:120] Setting up reverse
I0406 18:08:01.553078 86686 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.553082 86686 layer_factory.hpp:74] Creating layer concat
I0406 18:08:01.553091 86686 net.cpp:90] Creating Layer concat
I0406 18:08:01.553095 86686 net.cpp:410] concat <- lstm_drop
I0406 18:08:01.553100 86686 net.cpp:410] concat <- inv_lstm_output
I0406 18:08:01.553104 86686 net.cpp:368] concat -> lstm_concat
I0406 18:08:01.553112 86686 net.cpp:120] Setting up concat
I0406 18:08:01.553123 86686 net.cpp:127] Top shape: 16 16 1024 (262144)
I0406 18:08:01.553128 86686 layer_factory.hpp:74] Creating layer fc8_final
I0406 18:08:01.553138 86686 net.cpp:90] Creating Layer fc8_final
I0406 18:08:01.553143 86686 net.cpp:410] fc8_final <- lstm_concat
I0406 18:08:01.553149 86686 net.cpp:368] fc8_final -> fc8_final
I0406 18:08:01.553155 86686 net.cpp:120] Setting up fc8_final
I0406 18:08:01.553578 86686 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:01.553586 86686 layer_factory.hpp:74] Creating layer fc8_final_fc8_final_0_split
I0406 18:08:01.553592 86686 net.cpp:90] Creating Layer fc8_final_fc8_final_0_split
I0406 18:08:01.553597 86686 net.cpp:410] fc8_final_fc8_final_0_split <- fc8_final
I0406 18:08:01.553602 86686 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_0
I0406 18:08:01.553609 86686 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_1
I0406 18:08:01.553617 86686 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_2
I0406 18:08:01.553622 86686 net.cpp:120] Setting up fc8_final_fc8_final_0_split
I0406 18:08:01.553627 86686 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:01.553632 86686 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:01.553635 86686 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:01.553648 86686 layer_factory.hpp:74] Creating layer loss
I0406 18:08:01.553658 86686 net.cpp:90] Creating Layer loss
I0406 18:08:01.553663 86686 net.cpp:410] loss <- fc8_final_fc8_final_0_split_0
I0406 18:08:01.553668 86686 net.cpp:410] loss <- label_data_1_split_0
I0406 18:08:01.553673 86686 net.cpp:368] loss -> loss
I0406 18:08:01.553683 86686 net.cpp:120] Setting up loss
I0406 18:08:01.553690 86686 layer_factory.hpp:74] Creating layer loss
I0406 18:08:01.554126 86686 net.cpp:127] Top shape: (1)
I0406 18:08:01.554137 86686 net.cpp:129]     with loss weight 1
I0406 18:08:01.554149 86686 layer_factory.hpp:74] Creating layer accuracy
I0406 18:08:01.554157 86686 net.cpp:90] Creating Layer accuracy
I0406 18:08:01.554160 86686 net.cpp:410] accuracy <- fc8_final_fc8_final_0_split_1
I0406 18:08:01.554167 86686 net.cpp:410] accuracy <- label_data_1_split_1
I0406 18:08:01.554173 86686 net.cpp:368] accuracy -> accuracy
I0406 18:08:01.554180 86686 net.cpp:120] Setting up accuracy
I0406 18:08:01.554186 86686 net.cpp:127] Top shape: (1)
I0406 18:08:01.554189 86686 layer_factory.hpp:74] Creating layer accuracy_top2
I0406 18:08:01.554198 86686 net.cpp:90] Creating Layer accuracy_top2
I0406 18:08:01.554200 86686 net.cpp:410] accuracy_top2 <- fc8_final_fc8_final_0_split_2
I0406 18:08:01.554204 86686 net.cpp:410] accuracy_top2 <- label_data_1_split_2
I0406 18:08:01.554209 86686 net.cpp:368] accuracy_top2 -> accuracy_top2
I0406 18:08:01.554214 86686 net.cpp:120] Setting up accuracy_top2
I0406 18:08:01.554219 86686 net.cpp:127] Top shape: (1)
I0406 18:08:01.554222 86686 net.cpp:194] accuracy_top2 does not need backward computation.
I0406 18:08:01.554226 86686 net.cpp:194] accuracy does not need backward computation.
I0406 18:08:01.554230 86686 net.cpp:192] loss needs backward computation.
I0406 18:08:01.554234 86686 net.cpp:192] fc8_final_fc8_final_0_split needs backward computation.
I0406 18:08:01.554237 86686 net.cpp:192] fc8_final needs backward computation.
I0406 18:08:01.554241 86686 net.cpp:192] concat needs backward computation.
I0406 18:08:01.554245 86686 net.cpp:192] reverse needs backward computation.
I0406 18:08:01.554251 86686 net.cpp:192] inv_lstm_drop needs backward computation.
I0406 18:08:01.554255 86686 net.cpp:192] invlstm needs backward computation.
I0406 18:08:01.554260 86686 net.cpp:192] reverse needs backward computation.
I0406 18:08:01.554263 86686 net.cpp:192] lstm_drop needs backward computation.
I0406 18:08:01.554267 86686 net.cpp:192] lstm needs backward computation.
I0406 18:08:01.554271 86686 net.cpp:192] data_reshape_reshape_data_0_split needs backward computation.
I0406 18:08:01.554275 86686 net.cpp:192] reshape_data needs backward computation.
I0406 18:08:01.554280 86686 net.cpp:192] drop6 needs backward computation.
I0406 18:08:01.554282 86686 net.cpp:192] relu6 needs backward computation.
I0406 18:08:01.554286 86686 net.cpp:192] fc6 needs backward computation.
I0406 18:08:01.554291 86686 net.cpp:192] pool5 needs backward computation.
I0406 18:08:01.554296 86686 net.cpp:192] relu5 needs backward computation.
I0406 18:08:01.554299 86686 net.cpp:192] bn5 needs backward computation.
I0406 18:08:01.554304 86686 net.cpp:192] conv5 needs backward computation.
I0406 18:08:01.554311 86686 net.cpp:192] relu4 needs backward computation.
I0406 18:08:01.554316 86686 net.cpp:192] bn4 needs backward computation.
I0406 18:08:01.554321 86686 net.cpp:192] conv4 needs backward computation.
I0406 18:08:01.554325 86686 net.cpp:192] relu3 needs backward computation.
I0406 18:08:01.554329 86686 net.cpp:192] bn3 needs backward computation.
I0406 18:08:01.554333 86686 net.cpp:192] conv3 needs backward computation.
I0406 18:08:01.554338 86686 net.cpp:192] norm2 needs backward computation.
I0406 18:08:01.554342 86686 net.cpp:192] pool2 needs backward computation.
I0406 18:08:01.554347 86686 net.cpp:192] relu2 needs backward computation.
I0406 18:08:01.554350 86686 net.cpp:192] bn2 needs backward computation.
I0406 18:08:01.554354 86686 net.cpp:192] conv2 needs backward computation.
I0406 18:08:01.554358 86686 net.cpp:192] norm1 needs backward computation.
I0406 18:08:01.554374 86686 net.cpp:192] pool1 needs backward computation.
I0406 18:08:01.554378 86686 net.cpp:192] relu1 needs backward computation.
I0406 18:08:01.554383 86686 net.cpp:192] bn1 needs backward computation.
I0406 18:08:01.554386 86686 net.cpp:192] conv1 needs backward computation.
I0406 18:08:01.554391 86686 net.cpp:194] clip_marker_data_2_split does not need backward computation.
I0406 18:08:01.554395 86686 net.cpp:194] label_data_1_split does not need backward computation.
I0406 18:08:01.554400 86686 net.cpp:194] data does not need backward computation.
I0406 18:08:01.554404 86686 net.cpp:235] This network produces output accuracy
I0406 18:08:01.554409 86686 net.cpp:235] This network produces output accuracy_top2
I0406 18:08:01.554411 86686 net.cpp:235] This network produces output loss
I0406 18:08:01.554442 86686 net.cpp:482] Collecting Learning Rate and Weight Decay.
I0406 18:08:01.554455 86686 net.cpp:247] Network initialization done.
I0406 18:08:01.554460 86686 net.cpp:248] Memory required for data: 6095228940
I0406 18:08:01.555322 86685 net.cpp:127] Top shape: 256 4096 (1048576)
I0406 18:08:01.555357 86685 layer_factory.hpp:74] Creating layer relu6
I0406 18:08:01.555374 86685 net.cpp:90] Creating Layer relu6
I0406 18:08:01.555383 86685 net.cpp:410] relu6 <- fc6
I0406 18:08:01.555392 86685 net.cpp:357] relu6 -> fc6 (in-place)
I0406 18:08:01.555398 86685 net.cpp:120] Setting up relu6
I0406 18:08:01.555853 86686 solver.cpp:170] Creating test net (#0) specified by net file: train_test_lstm.prototxt
I0406 18:08:01.555912 86685 net.cpp:127] Top shape: 256 4096 (1048576)
I0406 18:08:01.555919 86685 layer_factory.hpp:74] Creating layer drop6
I0406 18:08:01.555940 86685 net.cpp:90] Creating Layer drop6
I0406 18:08:01.555944 86685 net.cpp:410] drop6 <- fc6
I0406 18:08:01.555951 86685 net.cpp:357] drop6 -> fc6 (in-place)
I0406 18:08:01.555954 86686 net.cpp:287] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0406 18:08:01.555963 86685 net.cpp:120] Setting up drop6
I0406 18:08:01.555971 86685 net.cpp:127] Top shape: 256 4096 (1048576)
I0406 18:08:01.555975 86685 layer_factory.hpp:74] Creating layer reshape_data
I0406 18:08:01.555986 86685 net.cpp:90] Creating Layer reshape_data
I0406 18:08:01.555992 86685 net.cpp:410] reshape_data <- fc6
I0406 18:08:01.556000 86685 net.cpp:368] reshape_data -> data_reshape
I0406 18:08:01.556010 86685 net.cpp:120] Setting up reshape_data
I0406 18:08:01.556017 86685 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.556021 86685 layer_factory.hpp:74] Creating layer data_reshape_reshape_data_0_split
I0406 18:08:01.556026 86685 net.cpp:90] Creating Layer data_reshape_reshape_data_0_split
I0406 18:08:01.556030 86685 net.cpp:410] data_reshape_reshape_data_0_split <- data_reshape
I0406 18:08:01.556036 86685 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_0
I0406 18:08:01.556044 86685 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_1
I0406 18:08:01.556051 86685 net.cpp:120] Setting up data_reshape_reshape_data_0_split
I0406 18:08:01.556056 86685 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.556061 86685 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.556063 86685 layer_factory.hpp:74] Creating layer lstm
I0406 18:08:01.556076 86685 net.cpp:90] Creating Layer lstm
I0406 18:08:01.556082 86685 net.cpp:410] lstm <- data_reshape_reshape_data_0_split_0
I0406 18:08:01.556087 86685 net.cpp:410] lstm <- clip_marker_data_2_split_0
I0406 18:08:01.556098 86685 net.cpp:368] lstm -> lstm
I0406 18:08:01.556121 86685 net.cpp:120] Setting up lstm
I0406 18:08:01.556257 86686 net.cpp:42] Initializing net from parameters: 
name: "lstm_joints"
state {
  phase: TEST
  stage: "test-on-test"
}
layer {
  name: "data"
  type: "VideoDataRNN"
  top: "data"
  top: "label"
  top: "clip_marker"
  include {
    phase: TEST
    stage: "test-on-test"
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
    fix_crop: true
    is_flow: false
  }
  video_data_param {
    source: "/data2/action/rgb/list/test_video.txt"
    batch_size: 2
    shuffle: true
    new_height: 240
    new_width: 320
    modality: RGB
    root_folder: "/data2/action/rgb/"
    seq_length: 16
    stride_seq: 6
    eval_last: true
  }
}
layer {
  name: "conv1"
  type: "ConvolutionData"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn1"
  type: "BNData"
  bottom: "conv1"
  top: "conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1/bn"
  top: "conv1/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "ConvolutionData"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 5
    group: 2
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn2"
  type: "BNData"
  bottom: "conv2"
  top: "conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2/bn"
  top: "conv2/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "ConvolutionData"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn3"
  type: "BNData"
  bottom: "conv3"
  top: "conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3/bn"
  top: "conv3/bn"
}
layer {
  name: "conv4"
  type: "ConvolutionData"
  bottom: "conv3/bn"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn4"
  type: "BNData"
  bottom: "conv4"
  top: "conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4/bn"
  top: "conv4/bn"
}
layer {
  name: "conv5"
  type: "ConvolutionData"
  bottom: "conv4/bn"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn5"
  type: "BNData"
  bottom: "conv5"
  top: "conv5/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5/bn"
  top: "conv5/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5/bn"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reshape_data"
  type: "Reshape"
  bottom: "fc6"
  top: "data_reshape"
  reshape_param {
    shape {
      dim: 16
      dim: -1
      dim: 4096
    }
  }
}
layer {
  name: "lstm"
  type: "SLLSTM"
  bottom: "data_reshape"
  bottom: "clip_marker"
  top: "lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "lstm_drop"
  type: "Dropout"
  bottom: "lstm"
  top: "lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "data_reshape"
  top: "inv_lstm_input"
}
layer {
  name: "invlstm"
  type: "SLLSTM"
  bottom: "inv_lstm_input"
  bottom: "clip_marker"
  top: "inv_lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "inv_lstm_drop"
  type: "Dropout"
  bottom: "inv_lstm"
  top: "inv_lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "inv_lstm_drop"
  top: "inv_lstm_output"
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "lstm_drop"
  bottom: "inv_lstm_output"
  top: "lstm_concat"
  concat_param {
    axis: 2
  }
}
layer {
  name: "fc8_final"
  type: "InnerProduct"
  bottom: "lstm_concat"
  top: "fc8_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 12
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_final"
  bottom: "label"
  top: "loss"
  loss_param {
    ignore_label: -1
  }
  softmax_param {
    axis: 2
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy"
  accuracy_param {
    axis: 2
    ignore_label: -1
  }
}
layer {
  name: "accuracy_top2"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy_top2"
  accuracy_param {
    top_k: 2
    axis: 2
    ignore_label: -1
  }
}
I0406 18:08:01.556453 86686 layer_factory.hpp:74] Creating layer data
I0406 18:08:01.556468 86686 net.cpp:90] Creating Layer data
I0406 18:08:01.556475 86686 net.cpp:368] data -> data
I0406 18:08:01.556485 86686 net.cpp:368] data -> label
I0406 18:08:01.556494 86686 net.cpp:368] data -> clip_marker
I0406 18:08:01.556500 86686 net.cpp:120] Setting up data
I0406 18:08:01.556506 86686 video_data_rnn_layer.cpp:42] Opening file: /data2/action/rgb/list/test_video.txt
I0406 18:08:01.556627 86686 video_data_rnn_layer.cpp:58] A total of 180 videos.
I0406 18:08:01.562860 86686 net.cpp:127] Top shape: 16 3 227 227 (2473392)
I0406 18:08:01.562909 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.562918 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.562928 86686 layer_factory.hpp:74] Creating layer label_data_1_split
I0406 18:08:01.562945 86686 net.cpp:90] Creating Layer label_data_1_split
I0406 18:08:01.562952 86686 net.cpp:410] label_data_1_split <- label
I0406 18:08:01.562965 86686 net.cpp:368] label_data_1_split -> label_data_1_split_0
I0406 18:08:01.563002 86686 net.cpp:368] label_data_1_split -> label_data_1_split_1
I0406 18:08:01.563019 86686 net.cpp:368] label_data_1_split -> label_data_1_split_2
I0406 18:08:01.563030 86686 net.cpp:120] Setting up label_data_1_split
I0406 18:08:01.563048 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563062 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563072 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563083 86686 layer_factory.hpp:74] Creating layer clip_marker_data_2_split
I0406 18:08:01.563097 86686 net.cpp:90] Creating Layer clip_marker_data_2_split
I0406 18:08:01.563107 86686 net.cpp:410] clip_marker_data_2_split <- clip_marker
I0406 18:08:01.563118 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_0
I0406 18:08:01.563160 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_1
I0406 18:08:01.563177 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_2
I0406 18:08:01.563194 86686 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_3
I0406 18:08:01.563208 86686 net.cpp:120] Setting up clip_marker_data_2_split
I0406 18:08:01.563223 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563236 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563246 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563257 86686 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:01.563266 86686 layer_factory.hpp:74] Creating layer conv1
I0406 18:08:01.563283 86686 net.cpp:90] Creating Layer conv1
I0406 18:08:01.563294 86686 net.cpp:410] conv1 <- data
I0406 18:08:01.563308 86686 net.cpp:368] conv1 -> conv1
I0406 18:08:01.563325 86686 net.cpp:120] Setting up conv1
I0406 18:08:01.825242 86685 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.825291 86685 layer_factory.hpp:74] Creating layer lstm_drop
I0406 18:08:01.825304 86685 net.cpp:90] Creating Layer lstm_drop
I0406 18:08:01.825309 86685 net.cpp:410] lstm_drop <- lstm
I0406 18:08:01.825317 86685 net.cpp:368] lstm_drop -> lstm_drop
I0406 18:08:01.825327 86685 net.cpp:120] Setting up lstm_drop
I0406 18:08:01.825347 86685 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:01.825358 86685 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:01.825366 86685 net.cpp:90] Creating Layer reverse
I0406 18:08:01.825371 86685 net.cpp:410] reverse <- clip_marker_data_2_split_1
I0406 18:08:01.825376 86685 net.cpp:410] reverse <- data_reshape_reshape_data_0_split_1
I0406 18:08:01.825382 86685 net.cpp:368] reverse -> inv_lstm_input
I0406 18:08:01.825388 86685 net.cpp:120] Setting up reverse
I0406 18:08:01.825394 86685 net.cpp:127] Top shape: 16 16 4096 (1048576)
I0406 18:08:01.825397 86685 layer_factory.hpp:74] Creating layer invlstm
I0406 18:08:01.825420 86685 net.cpp:90] Creating Layer invlstm
I0406 18:08:01.825426 86685 net.cpp:410] invlstm <- inv_lstm_input
I0406 18:08:01.825431 86685 net.cpp:410] invlstm <- clip_marker_data_2_split_2
I0406 18:08:01.825438 86685 net.cpp:368] invlstm -> inv_lstm
I0406 18:08:01.825448 86685 net.cpp:120] Setting up invlstm
I0406 18:08:02.086694 86685 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:02.086736 86685 layer_factory.hpp:74] Creating layer inv_lstm_drop
I0406 18:08:02.086750 86685 net.cpp:90] Creating Layer inv_lstm_drop
I0406 18:08:02.086756 86685 net.cpp:410] inv_lstm_drop <- inv_lstm
I0406 18:08:02.086766 86685 net.cpp:368] inv_lstm_drop -> inv_lstm_drop
I0406 18:08:02.086776 86685 net.cpp:120] Setting up inv_lstm_drop
I0406 18:08:02.086796 86685 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:02.086805 86685 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:02.086812 86685 net.cpp:90] Creating Layer reverse
I0406 18:08:02.086818 86685 net.cpp:410] reverse <- clip_marker_data_2_split_3
I0406 18:08:02.086823 86685 net.cpp:410] reverse <- inv_lstm_drop
I0406 18:08:02.086829 86685 net.cpp:368] reverse -> inv_lstm_output
I0406 18:08:02.086834 86685 net.cpp:120] Setting up reverse
I0406 18:08:02.086840 86685 net.cpp:127] Top shape: 16 16 512 (131072)
I0406 18:08:02.086843 86685 layer_factory.hpp:74] Creating layer concat
I0406 18:08:02.086850 86685 net.cpp:90] Creating Layer concat
I0406 18:08:02.086869 86685 net.cpp:410] concat <- lstm_drop
I0406 18:08:02.086885 86685 net.cpp:410] concat <- inv_lstm_output
I0406 18:08:02.086891 86685 net.cpp:368] concat -> lstm_concat
I0406 18:08:02.086900 86685 net.cpp:120] Setting up concat
I0406 18:08:02.086921 86685 net.cpp:127] Top shape: 16 16 1024 (262144)
I0406 18:08:02.086927 86685 layer_factory.hpp:74] Creating layer fc8_final
I0406 18:08:02.086937 86685 net.cpp:90] Creating Layer fc8_final
I0406 18:08:02.086942 86685 net.cpp:410] fc8_final <- lstm_concat
I0406 18:08:02.086948 86685 net.cpp:368] fc8_final -> fc8_final
I0406 18:08:02.086956 86685 net.cpp:120] Setting up fc8_final
I0406 18:08:02.087371 86685 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:02.087379 86685 layer_factory.hpp:74] Creating layer fc8_final_fc8_final_0_split
I0406 18:08:02.087398 86685 net.cpp:90] Creating Layer fc8_final_fc8_final_0_split
I0406 18:08:02.087401 86685 net.cpp:410] fc8_final_fc8_final_0_split <- fc8_final
I0406 18:08:02.087407 86685 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_0
I0406 18:08:02.087415 86685 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_1
I0406 18:08:02.087422 86685 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_2
I0406 18:08:02.087440 86685 net.cpp:120] Setting up fc8_final_fc8_final_0_split
I0406 18:08:02.087452 86685 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:02.087456 86685 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:02.087460 86685 net.cpp:127] Top shape: 16 16 12 (3072)
I0406 18:08:02.087491 86685 layer_factory.hpp:74] Creating layer loss
I0406 18:08:02.087501 86685 net.cpp:90] Creating Layer loss
I0406 18:08:02.087504 86685 net.cpp:410] loss <- fc8_final_fc8_final_0_split_0
I0406 18:08:02.087520 86685 net.cpp:410] loss <- label_data_1_split_0
I0406 18:08:02.087527 86685 net.cpp:368] loss -> loss
I0406 18:08:02.087549 86685 net.cpp:120] Setting up loss
I0406 18:08:02.087558 86685 layer_factory.hpp:74] Creating layer loss
I0406 18:08:02.088152 86685 net.cpp:127] Top shape: (1)
I0406 18:08:02.088163 86685 net.cpp:129]     with loss weight 1
I0406 18:08:02.088186 86685 layer_factory.hpp:74] Creating layer accuracy
I0406 18:08:02.088194 86685 net.cpp:90] Creating Layer accuracy
I0406 18:08:02.088198 86685 net.cpp:410] accuracy <- fc8_final_fc8_final_0_split_1
I0406 18:08:02.088203 86685 net.cpp:410] accuracy <- label_data_1_split_1
I0406 18:08:02.088223 86685 net.cpp:368] accuracy -> accuracy
I0406 18:08:02.088232 86685 net.cpp:120] Setting up accuracy
I0406 18:08:02.088239 86685 net.cpp:127] Top shape: (1)
I0406 18:08:02.088243 86685 layer_factory.hpp:74] Creating layer accuracy_top2
I0406 18:08:02.088249 86685 net.cpp:90] Creating Layer accuracy_top2
I0406 18:08:02.088253 86685 net.cpp:410] accuracy_top2 <- fc8_final_fc8_final_0_split_2
I0406 18:08:02.088258 86685 net.cpp:410] accuracy_top2 <- label_data_1_split_2
I0406 18:08:02.088276 86685 net.cpp:368] accuracy_top2 -> accuracy_top2
I0406 18:08:02.088282 86685 net.cpp:120] Setting up accuracy_top2
I0406 18:08:02.088287 86685 net.cpp:127] Top shape: (1)
I0406 18:08:02.088290 86685 net.cpp:194] accuracy_top2 does not need backward computation.
I0406 18:08:02.088305 86685 net.cpp:194] accuracy does not need backward computation.
I0406 18:08:02.088310 86685 net.cpp:192] loss needs backward computation.
I0406 18:08:02.088313 86685 net.cpp:192] fc8_final_fc8_final_0_split needs backward computation.
I0406 18:08:02.088328 86685 net.cpp:192] fc8_final needs backward computation.
I0406 18:08:02.088333 86685 net.cpp:192] concat needs backward computation.
I0406 18:08:02.088337 86685 net.cpp:192] reverse needs backward computation.
I0406 18:08:02.088341 86685 net.cpp:192] inv_lstm_drop needs backward computation.
I0406 18:08:02.088346 86685 net.cpp:192] invlstm needs backward computation.
I0406 18:08:02.088351 86685 net.cpp:192] reverse needs backward computation.
I0406 18:08:02.088356 86685 net.cpp:192] lstm_drop needs backward computation.
I0406 18:08:02.088359 86685 net.cpp:192] lstm needs backward computation.
I0406 18:08:02.088363 86685 net.cpp:192] data_reshape_reshape_data_0_split needs backward computation.
I0406 18:08:02.088367 86685 net.cpp:192] reshape_data needs backward computation.
I0406 18:08:02.088371 86685 net.cpp:192] drop6 needs backward computation.
I0406 18:08:02.088376 86685 net.cpp:192] relu6 needs backward computation.
I0406 18:08:02.088379 86685 net.cpp:192] fc6 needs backward computation.
I0406 18:08:02.088384 86685 net.cpp:192] pool5 needs backward computation.
I0406 18:08:02.088390 86685 net.cpp:192] relu5 needs backward computation.
I0406 18:08:02.088394 86685 net.cpp:192] bn5 needs backward computation.
I0406 18:08:02.088399 86685 net.cpp:192] conv5 needs backward computation.
I0406 18:08:02.088405 86685 net.cpp:192] relu4 needs backward computation.
I0406 18:08:02.088410 86685 net.cpp:192] bn4 needs backward computation.
I0406 18:08:02.088415 86685 net.cpp:192] conv4 needs backward computation.
I0406 18:08:02.088420 86685 net.cpp:192] relu3 needs backward computation.
I0406 18:08:02.088426 86685 net.cpp:192] bn3 needs backward computation.
I0406 18:08:02.088430 86685 net.cpp:192] conv3 needs backward computation.
I0406 18:08:02.088435 86685 net.cpp:192] norm2 needs backward computation.
I0406 18:08:02.088439 86685 net.cpp:192] pool2 needs backward computation.
I0406 18:08:02.088443 86685 net.cpp:192] relu2 needs backward computation.
I0406 18:08:02.088449 86685 net.cpp:192] bn2 needs backward computation.
I0406 18:08:02.088464 86685 net.cpp:192] conv2 needs backward computation.
I0406 18:08:02.088467 86685 net.cpp:192] norm1 needs backward computation.
I0406 18:08:02.088484 86685 net.cpp:192] pool1 needs backward computation.
I0406 18:08:02.088490 86685 net.cpp:192] relu1 needs backward computation.
I0406 18:08:02.088493 86685 net.cpp:192] bn1 needs backward computation.
I0406 18:08:02.088498 86685 net.cpp:192] conv1 needs backward computation.
I0406 18:08:02.088503 86685 net.cpp:194] clip_marker_data_2_split does not need backward computation.
I0406 18:08:02.088508 86685 net.cpp:194] label_data_1_split does not need backward computation.
I0406 18:08:02.088513 86685 net.cpp:194] data does not need backward computation.
I0406 18:08:02.088516 86685 net.cpp:235] This network produces output accuracy
I0406 18:08:02.088521 86685 net.cpp:235] This network produces output accuracy_top2
I0406 18:08:02.088524 86685 net.cpp:235] This network produces output loss
I0406 18:08:02.088554 86685 net.cpp:482] Collecting Learning Rate and Weight Decay.
I0406 18:08:02.088567 86685 net.cpp:247] Network initialization done.
I0406 18:08:02.088572 86685 net.cpp:248] Memory required for data: 6095228940
I0406 18:08:02.090030 86685 solver.cpp:170] Creating test net (#0) specified by net file: train_test_lstm.prototxt
I0406 18:08:02.090134 86685 net.cpp:287] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0406 18:08:02.090504 86685 net.cpp:42] Initializing net from parameters: 
name: "lstm_joints"
state {
  phase: TEST
  stage: "test-on-test"
}
layer {
  name: "data"
  type: "VideoDataRNN"
  top: "data"
  top: "label"
  top: "clip_marker"
  include {
    phase: TEST
    stage: "test-on-test"
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
    fix_crop: true
    is_flow: false
  }
  video_data_param {
    source: "/data2/action/rgb/list/test_video.txt"
    batch_size: 2
    shuffle: true
    new_height: 240
    new_width: 320
    modality: RGB
    root_folder: "/data2/action/rgb/"
    seq_length: 16
    stride_seq: 6
    eval_last: true
  }
}
layer {
  name: "conv1"
  type: "ConvolutionData"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn1"
  type: "BNData"
  bottom: "conv1"
  top: "conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1/bn"
  top: "conv1/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "ConvolutionData"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 5
    group: 2
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn2"
  type: "BNData"
  bottom: "conv2"
  top: "conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2/bn"
  top: "conv2/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "ConvolutionData"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn3"
  type: "BNData"
  bottom: "conv3"
  top: "conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3/bn"
  top: "conv3/bn"
}
layer {
  name: "conv4"
  type: "ConvolutionData"
  bottom: "conv3/bn"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn4"
  type: "BNData"
  bottom: "conv4"
  top: "conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4/bn"
  top: "conv4/bn"
}
layer {
  name: "conv5"
  type: "ConvolutionData"
  bottom: "conv4/bn"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "bn5"
  type: "BNData"
  bottom: "conv5"
  top: "conv5/bn"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
    moving_average: true
    decay: 0.05
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5/bn"
  top: "conv5/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5/bn"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reshape_data"
  type: "Reshape"
  bottom: "fc6"
  top: "data_reshape"
  reshape_param {
    shape {
      dim: 16
      dim: -1
      dim: 4096
    }
  }
}
layer {
  name: "lstm"
  type: "SLLSTM"
  bottom: "data_reshape"
  bottom: "clip_marker"
  top: "lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "lstm_drop"
  type: "Dropout"
  bottom: "lstm"
  top: "lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "data_reshape"
  top: "inv_lstm_input"
}
layer {
  name: "invlstm"
  type: "SLLSTM"
  bottom: "inv_lstm_input"
  bottom: "clip_marker"
  top: "inv_lstm"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  recurrent_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "inv_lstm_drop"
  type: "Dropout"
  bottom: "inv_lstm"
  top: "inv_lstm_drop"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "reverse"
  type: "Reverse"
  bottom: "clip_marker"
  bottom: "inv_lstm_drop"
  top: "inv_lstm_output"
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "lstm_drop"
  bottom: "inv_lstm_output"
  top: "lstm_concat"
  concat_param {
    axis: 2
  }
}
layer {
  name: "fc8_final"
  type: "InnerProduct"
  bottom: "lstm_concat"
  top: "fc8_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 12
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_final"
  bottom: "label"
  top: "loss"
  loss_param {
    ignore_label: -1
  }
  softmax_param {
    axis: 2
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy"
  accuracy_param {
    axis: 2
    ignore_label: -1
  }
}
layer {
  name: "accuracy_top2"
  type: "Accuracy"
  bottom: "fc8_final"
  bottom: "label"
  top: "accuracy_top2"
  accuracy_param {
    top_k: 2
    axis: 2
    ignore_label: -1
  }
}
I0406 18:08:02.090715 86685 layer_factory.hpp:74] Creating layer data
I0406 18:08:02.090741 86685 net.cpp:90] Creating Layer data
I0406 18:08:02.090747 86685 net.cpp:368] data -> data
I0406 18:08:02.090771 86685 net.cpp:368] data -> label
I0406 18:08:02.090790 86685 net.cpp:368] data -> clip_marker
I0406 18:08:02.090808 86685 net.cpp:120] Setting up data
I0406 18:08:02.090814 86685 video_data_rnn_layer.cpp:42] Opening file: /data2/action/rgb/list/test_video.txt
I0406 18:08:02.090966 86685 video_data_rnn_layer.cpp:58] A total of 180 videos.
I0406 18:08:02.097151 86685 net.cpp:127] Top shape: 16 3 227 227 (2473392)
I0406 18:08:02.097178 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097187 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097193 86685 layer_factory.hpp:74] Creating layer label_data_1_split
I0406 18:08:02.097215 86685 net.cpp:90] Creating Layer label_data_1_split
I0406 18:08:02.097223 86685 net.cpp:410] label_data_1_split <- label
I0406 18:08:02.097244 86685 net.cpp:368] label_data_1_split -> label_data_1_split_0
I0406 18:08:02.097266 86685 net.cpp:368] label_data_1_split -> label_data_1_split_1
I0406 18:08:02.097297 86685 net.cpp:368] label_data_1_split -> label_data_1_split_2
I0406 18:08:02.097311 86685 net.cpp:120] Setting up label_data_1_split
I0406 18:08:02.097340 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097349 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097357 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097363 86685 layer_factory.hpp:74] Creating layer clip_marker_data_2_split
I0406 18:08:02.097373 86685 net.cpp:90] Creating Layer clip_marker_data_2_split
I0406 18:08:02.097380 86685 net.cpp:410] clip_marker_data_2_split <- clip_marker
I0406 18:08:02.097390 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_0
I0406 18:08:02.097421 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_1
I0406 18:08:02.097436 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_2
I0406 18:08:02.097451 86685 net.cpp:368] clip_marker_data_2_split -> clip_marker_data_2_split_3
I0406 18:08:02.097465 86685 net.cpp:120] Setting up clip_marker_data_2_split
I0406 18:08:02.097476 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097484 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097492 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097501 86685 net.cpp:127] Top shape: 16 1 1 (16)
I0406 18:08:02.097506 86685 layer_factory.hpp:74] Creating layer conv1
I0406 18:08:02.097518 86685 net.cpp:90] Creating Layer conv1
I0406 18:08:02.097527 86685 net.cpp:410] conv1 <- data
I0406 18:08:02.097537 86685 net.cpp:368] conv1 -> conv1
I0406 18:08:02.097551 86685 net.cpp:120] Setting up conv1
I0406 18:08:02.099385 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 221796
I0406 18:08:02.099447 86686 net.cpp:127] Top shape: 16 96 111 111 (18925056)
I0406 18:08:02.099483 86686 layer_factory.hpp:74] Creating layer bn1
I0406 18:08:02.099495 86686 net.cpp:90] Creating Layer bn1
I0406 18:08:02.099501 86686 net.cpp:410] bn1 <- conv1
I0406 18:08:02.099509 86686 net.cpp:368] bn1 -> conv1/bn
I0406 18:08:02.099519 86686 net.cpp:120] Setting up bn1
I0406 18:08:02.099658 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 221796
I0406 18:08:02.099728 86685 net.cpp:127] Top shape: 16 96 111 111 (18925056)
I0406 18:08:02.099746 86685 layer_factory.hpp:74] Creating layer bn1
I0406 18:08:02.099773 86685 net.cpp:90] Creating Layer bn1
I0406 18:08:02.099781 86685 net.cpp:410] bn1 <- conv1
I0406 18:08:02.099792 86685 net.cpp:368] bn1 -> conv1/bn
I0406 18:08:02.099805 86685 net.cpp:120] Setting up bn1
I0406 18:08:02.099844 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.099875 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.099911 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.099923 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.099844 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.099875 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.099911 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.099922 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.099930 86686 net.cpp:127] Top shape: 16 96 111 111 (18925056)
I0406 18:08:02.099932 86685 net.cpp:127] Top shape: 16 96 111 111 (18925056)
I0406 18:08:02.099939 86686 layer_factory.hpp:74] Creating layer relu1
I0406 18:08:02.099958 86686 net.cpp:90] Creating Layer relu1
I0406 18:08:02.099961 86685 layer_factory.hpp:74] Creating layer relu1
I0406 18:08:02.099962 86686 net.cpp:410] relu1 <- conv1/bn
I0406 18:08:02.099968 86686 net.cpp:357] relu1 -> conv1/bn (in-place)
I0406 18:08:02.099969 86685 net.cpp:90] Creating Layer relu1
I0406 18:08:02.099974 86686 net.cpp:120] Setting up relu1
I0406 18:08:02.099975 86685 net.cpp:410] relu1 <- conv1/bn
I0406 18:08:02.099983 86685 net.cpp:357] relu1 -> conv1/bn (in-place)
I0406 18:08:02.099994 86685 net.cpp:120] Setting up relu1
I0406 18:08:02.100153 86686 net.cpp:127] Top shape: 16 96 111 111 (18925056)
I0406 18:08:02.100162 86686 layer_factory.hpp:74] Creating layer pool1
I0406 18:08:02.100181 86686 net.cpp:90] Creating Layer pool1
I0406 18:08:02.100185 86686 net.cpp:410] pool1 <- conv1/bn
I0406 18:08:02.100190 86686 net.cpp:368] pool1 -> pool1
I0406 18:08:02.100198 86686 net.cpp:120] Setting up pool1
I0406 18:08:02.100253 86685 net.cpp:127] Top shape: 16 96 111 111 (18925056)
I0406 18:08:02.100265 86685 layer_factory.hpp:74] Creating layer pool1
I0406 18:08:02.100276 86685 net.cpp:90] Creating Layer pool1
I0406 18:08:02.100282 86685 net.cpp:410] pool1 <- conv1/bn
I0406 18:08:02.100291 86685 net.cpp:368] pool1 -> pool1
I0406 18:08:02.100301 86685 net.cpp:120] Setting up pool1
I0406 18:08:02.100507 86686 net.cpp:127] Top shape: 16 96 55 55 (4646400)
I0406 18:08:02.100519 86686 layer_factory.hpp:74] Creating layer norm1
I0406 18:08:02.100528 86686 net.cpp:90] Creating Layer norm1
I0406 18:08:02.100531 86686 net.cpp:410] norm1 <- pool1
I0406 18:08:02.100538 86686 net.cpp:368] norm1 -> norm1
I0406 18:08:02.100543 86686 net.cpp:120] Setting up norm1
I0406 18:08:02.100550 86686 net.cpp:127] Top shape: 16 96 55 55 (4646400)
I0406 18:08:02.100554 86686 layer_factory.hpp:74] Creating layer conv2
I0406 18:08:02.100564 86686 net.cpp:90] Creating Layer conv2
I0406 18:08:02.100569 86686 net.cpp:410] conv2 <- norm1
I0406 18:08:02.100575 86686 net.cpp:368] conv2 -> conv2
I0406 18:08:02.100584 86686 net.cpp:120] Setting up conv2
I0406 18:08:02.100675 86685 net.cpp:127] Top shape: 16 96 55 55 (4646400)
I0406 18:08:02.100690 86685 layer_factory.hpp:74] Creating layer norm1
I0406 18:08:02.100702 86685 net.cpp:90] Creating Layer norm1
I0406 18:08:02.100709 86685 net.cpp:410] norm1 <- pool1
I0406 18:08:02.100718 86685 net.cpp:368] norm1 -> norm1
I0406 18:08:02.100730 86685 net.cpp:120] Setting up norm1
I0406 18:08:02.100744 86685 net.cpp:127] Top shape: 16 96 55 55 (4646400)
I0406 18:08:02.100751 86685 layer_factory.hpp:74] Creating layer conv2
I0406 18:08:02.100764 86685 net.cpp:90] Creating Layer conv2
I0406 18:08:02.100771 86685 net.cpp:410] conv2 <- norm1
I0406 18:08:02.100781 86685 net.cpp:368] conv2 -> conv2
I0406 18:08:02.100792 86685 net.cpp:120] Setting up conv2
I0406 18:08:02.121603 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 37392
I0406 18:08:02.121639 86686 net.cpp:127] Top shape: 16 384 26 26 (4153344)
I0406 18:08:02.121646 86686 layer_factory.hpp:74] Creating layer bn2
I0406 18:08:02.121659 86686 net.cpp:90] Creating Layer bn2
I0406 18:08:02.121662 86686 net.cpp:410] bn2 <- conv2
I0406 18:08:02.121680 86686 net.cpp:368] bn2 -> conv2/bn
I0406 18:08:02.121688 86686 net.cpp:120] Setting up bn2
I0406 18:08:02.122278 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 37392
I0406 18:08:02.122323 86685 net.cpp:127] Top shape: 16 384 26 26 (4153344)
I0406 18:08:02.122349 86685 layer_factory.hpp:74] Creating layer bn2
I0406 18:08:02.122361 86685 net.cpp:90] Creating Layer bn2
I0406 18:08:02.122372 86685 net.cpp:410] bn2 <- conv2
I0406 18:08:02.122386 86685 net.cpp:368] bn2 -> conv2/bn
I0406 18:08:02.122400 86685 net.cpp:120] Setting up bn2
I0406 18:08:02.122447 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.122475 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.122489 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.122447 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.122475 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.122488 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.122503 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.122503 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.122512 86686 net.cpp:127] Top shape: 16 384 26 26 (4153344)
I0406 18:08:02.122512 86685 net.cpp:127] Top shape: 16 384 26 26 (4153344)
I0406 18:08:02.122534 86686 layer_factory.hpp:74] Creating layer relu2
I0406 18:08:02.122555 86686 net.cpp:90] Creating Layer relu2
I0406 18:08:02.122540 86685 layer_factory.hpp:74] Creating layer relu2
I0406 18:08:02.122560 86686 net.cpp:410] relu2 <- conv2/bn
I0406 18:08:02.122565 86685 net.cpp:90] Creating Layer relu2
I0406 18:08:02.122566 86686 net.cpp:357] relu2 -> conv2/bn (in-place)
I0406 18:08:02.122572 86686 net.cpp:120] Setting up relu2
I0406 18:08:02.122572 86685 net.cpp:410] relu2 <- conv2/bn
I0406 18:08:02.122583 86685 net.cpp:357] relu2 -> conv2/bn (in-place)
I0406 18:08:02.122596 86685 net.cpp:120] Setting up relu2
I0406 18:08:02.122943 86686 net.cpp:127] Top shape: 16 384 26 26 (4153344)
I0406 18:08:02.122954 86686 layer_factory.hpp:74] Creating layer pool2
I0406 18:08:02.122974 86686 net.cpp:90] Creating Layer pool2
I0406 18:08:02.122979 86686 net.cpp:410] pool2 <- conv2/bn
I0406 18:08:02.122985 86686 net.cpp:368] pool2 -> pool2
I0406 18:08:02.123003 86686 net.cpp:120] Setting up pool2
I0406 18:08:02.123116 86685 net.cpp:127] Top shape: 16 384 26 26 (4153344)
I0406 18:08:02.123131 86685 layer_factory.hpp:74] Creating layer pool2
I0406 18:08:02.123154 86685 net.cpp:90] Creating Layer pool2
I0406 18:08:02.123162 86685 net.cpp:410] pool2 <- conv2/bn
I0406 18:08:02.123169 86685 net.cpp:368] pool2 -> pool2
I0406 18:08:02.123204 86685 net.cpp:120] Setting up pool2
I0406 18:08:02.123230 86686 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.123239 86686 layer_factory.hpp:74] Creating layer norm2
I0406 18:08:02.123247 86686 net.cpp:90] Creating Layer norm2
I0406 18:08:02.123251 86686 net.cpp:410] norm2 <- pool2
I0406 18:08:02.123257 86686 net.cpp:368] norm2 -> norm2
I0406 18:08:02.123265 86686 net.cpp:120] Setting up norm2
I0406 18:08:02.123272 86686 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.123276 86686 layer_factory.hpp:74] Creating layer conv3
I0406 18:08:02.123296 86686 net.cpp:90] Creating Layer conv3
I0406 18:08:02.123298 86686 net.cpp:410] conv3 <- norm2
I0406 18:08:02.123306 86686 net.cpp:368] conv3 -> conv3
I0406 18:08:02.123311 86686 net.cpp:120] Setting up conv3
I0406 18:08:02.123471 86685 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.123482 86685 layer_factory.hpp:74] Creating layer norm2
I0406 18:08:02.123505 86685 net.cpp:90] Creating Layer norm2
I0406 18:08:02.123512 86685 net.cpp:410] norm2 <- pool2
I0406 18:08:02.123531 86685 net.cpp:368] norm2 -> norm2
I0406 18:08:02.123541 86685 net.cpp:120] Setting up norm2
I0406 18:08:02.123564 86685 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.123579 86685 layer_factory.hpp:74] Creating layer conv3
I0406 18:08:02.123592 86685 net.cpp:90] Creating Layer conv3
I0406 18:08:02.123600 86685 net.cpp:410] conv3 <- norm2
I0406 18:08:02.123612 86685 net.cpp:368] conv3 -> conv3
I0406 18:08:02.123625 86685 net.cpp:120] Setting up conv3
I0406 18:08:02.191517 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 9588
I0406 18:08:02.191604 86686 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.191609 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 9588
I0406 18:08:02.191634 86686 layer_factory.hpp:74] Creating layer bn3
I0406 18:08:02.191649 86686 net.cpp:90] Creating Layer bn3
I0406 18:08:02.191668 86686 net.cpp:410] bn3 <- conv3
I0406 18:08:02.191679 86686 net.cpp:368] bn3 -> conv3/bn
I0406 18:08:02.191692 86686 net.cpp:120] Setting up bn3
I0406 18:08:02.191700 86685 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.191711 86685 layer_factory.hpp:74] Creating layer bn3
I0406 18:08:02.191722 86685 net.cpp:90] Creating Layer bn3
I0406 18:08:02.191732 86685 net.cpp:410] bn3 <- conv3
I0406 18:08:02.191741 86685 net.cpp:368] bn3 -> conv3/bn
I0406 18:08:02.191751 86685 net.cpp:120] Setting up bn3
I0406 18:08:02.191776 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.191776 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.191797 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.191802 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.191809 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.191813 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.191822 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.191823 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.191829 86685 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.191831 86686 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.191843 86685 layer_factory.hpp:74] Creating layer relu3
I0406 18:08:02.191843 86686 layer_factory.hpp:74] Creating layer relu3
I0406 18:08:02.191849 86685 net.cpp:90] Creating Layer relu3
I0406 18:08:02.191849 86686 net.cpp:90] Creating Layer relu3
I0406 18:08:02.191854 86686 net.cpp:410] relu3 <- conv3/bn
I0406 18:08:02.191854 86685 net.cpp:410] relu3 <- conv3/bn
I0406 18:08:02.191860 86685 net.cpp:357] relu3 -> conv3/bn (in-place)
I0406 18:08:02.191861 86686 net.cpp:357] relu3 -> conv3/bn (in-place)
I0406 18:08:02.191866 86685 net.cpp:120] Setting up relu3
I0406 18:08:02.191867 86686 net.cpp:120] Setting up relu3
I0406 18:08:02.192260 86685 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.192275 86685 layer_factory.hpp:74] Creating layer conv4
I0406 18:08:02.192299 86685 net.cpp:90] Creating Layer conv4
I0406 18:08:02.192303 86685 net.cpp:410] conv4 <- conv3/bn
I0406 18:08:02.192297 86686 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.192307 86686 layer_factory.hpp:74] Creating layer conv4
I0406 18:08:02.192311 86685 net.cpp:368] conv4 -> conv4
I0406 18:08:02.192329 86686 net.cpp:90] Creating Layer conv4
I0406 18:08:02.192330 86685 net.cpp:120] Setting up conv4
I0406 18:08:02.192334 86686 net.cpp:410] conv4 <- conv3/bn
I0406 18:08:02.192342 86686 net.cpp:368] conv4 -> conv4
I0406 18:08:02.192351 86686 net.cpp:120] Setting up conv4
I0406 18:08:02.227746 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:08:02.227784 86686 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.227792 86686 layer_factory.hpp:74] Creating layer bn4
I0406 18:08:02.227792 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:08:02.227802 86686 net.cpp:90] Creating Layer bn4
I0406 18:08:02.227819 86686 net.cpp:410] bn4 <- conv4
I0406 18:08:02.227831 86685 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.227828 86686 net.cpp:368] bn4 -> conv4/bn
I0406 18:08:02.227854 86686 net.cpp:120] Setting up bn4
I0406 18:08:02.227854 86685 layer_factory.hpp:74] Creating layer bn4
I0406 18:08:02.227864 86685 net.cpp:90] Creating Layer bn4
I0406 18:08:02.227874 86685 net.cpp:410] bn4 <- conv4
I0406 18:08:02.227880 86685 net.cpp:368] bn4 -> conv4/bn
I0406 18:08:02.227887 86685 net.cpp:120] Setting up bn4
I0406 18:08:02.227911 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.227912 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.227927 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.227931 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.227946 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.227942 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.227958 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.227958 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.227969 86685 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.227969 86686 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.227977 86685 layer_factory.hpp:74] Creating layer relu4
I0406 18:08:02.227977 86686 layer_factory.hpp:74] Creating layer relu4
I0406 18:08:02.227983 86685 net.cpp:90] Creating Layer relu4
I0406 18:08:02.227988 86685 net.cpp:410] relu4 <- conv4/bn
I0406 18:08:02.227984 86686 net.cpp:90] Creating Layer relu4
I0406 18:08:02.227988 86686 net.cpp:410] relu4 <- conv4/bn
I0406 18:08:02.227994 86686 net.cpp:357] relu4 -> conv4/bn (in-place)
I0406 18:08:02.227994 86685 net.cpp:357] relu4 -> conv4/bn (in-place)
I0406 18:08:02.228000 86686 net.cpp:120] Setting up relu4
I0406 18:08:02.228001 86685 net.cpp:120] Setting up relu4
I0406 18:08:02.228346 86686 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.228370 86686 layer_factory.hpp:74] Creating layer conv5
I0406 18:08:02.228392 86686 net.cpp:90] Creating Layer conv5
I0406 18:08:02.228396 86686 net.cpp:410] conv5 <- conv4/bn
I0406 18:08:02.228405 86686 net.cpp:368] conv5 -> conv5
I0406 18:08:02.228392 86685 net.cpp:127] Top shape: 16 512 13 13 (1384448)
I0406 18:08:02.228412 86686 net.cpp:120] Setting up conv5
I0406 18:08:02.228427 86685 layer_factory.hpp:74] Creating layer conv5
I0406 18:08:02.228451 86685 net.cpp:90] Creating Layer conv5
I0406 18:08:02.228458 86685 net.cpp:410] conv5 <- conv4/bn
I0406 18:08:02.228467 86685 net.cpp:368] conv5 -> conv5
I0406 18:08:02.228476 86685 net.cpp:120] Setting up conv5
I0406 18:08:02.255333 86686 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:08:02.255373 86686 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.255383 86686 layer_factory.hpp:74] Creating layer bn5
I0406 18:08:02.255393 86686 net.cpp:90] Creating Layer bn5
I0406 18:08:02.255409 86686 net.cpp:410] bn5 <- conv5
I0406 18:08:02.255416 86686 net.cpp:368] bn5 -> conv5/bn
I0406 18:08:02.255424 86686 net.cpp:120] Setting up bn5
I0406 18:08:02.255446 86685 cudnn_conv_data_layer.cpp:184] Reallocating workspace storage: 19176
I0406 18:08:02.255472 86685 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.255484 86685 layer_factory.hpp:74] Creating layer bn5
I0406 18:08:02.255496 86685 net.cpp:90] Creating Layer bn5
I0406 18:08:02.255504 86685 net.cpp:410] bn5 <- conv5
I0406 18:08:02.255511 86685 net.cpp:368] bn5 -> conv5/bn
I0406 18:08:02.255518 86685 net.cpp:120] Setting up bn5
I0406 18:08:02.255537 86685 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.255538 86686 bn_data_layer.cpp:60] scale: 1 1 1
I0406 18:08:02.255551 86686 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.255553 86685 bn_data_layer.cpp:71] shift: 0 0 0
I0406 18:08:02.255565 86686 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.255563 86685 bn_data_layer.cpp:80] mean: 0 0 0
I0406 18:08:02.255574 86686 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.255574 86685 bn_data_layer.cpp:89] variance: 0 0 0
I0406 18:08:02.255581 86685 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.255581 86686 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.255589 86685 layer_factory.hpp:74] Creating layer relu5
I0406 18:08:02.255589 86686 layer_factory.hpp:74] Creating layer relu5
I0406 18:08:02.255596 86685 net.cpp:90] Creating Layer relu5
I0406 18:08:02.255595 86686 net.cpp:90] Creating Layer relu5
I0406 18:08:02.255599 86686 net.cpp:410] relu5 <- conv5/bn
I0406 18:08:02.255599 86685 net.cpp:410] relu5 <- conv5/bn
I0406 18:08:02.255606 86686 net.cpp:357] relu5 -> conv5/bn (in-place)
I0406 18:08:02.255606 86685 net.cpp:357] relu5 -> conv5/bn (in-place)
I0406 18:08:02.255612 86685 net.cpp:120] Setting up relu5
I0406 18:08:02.255612 86686 net.cpp:120] Setting up relu5
I0406 18:08:02.255782 86685 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.255791 86685 layer_factory.hpp:74] Creating layer pool5
I0406 18:08:02.255811 86685 net.cpp:90] Creating Layer pool5
I0406 18:08:02.255816 86685 net.cpp:410] pool5 <- conv5/bn
I0406 18:08:02.255811 86686 net.cpp:127] Top shape: 16 384 13 13 (1038336)
I0406 18:08:02.255820 86686 layer_factory.hpp:74] Creating layer pool5
I0406 18:08:02.255822 86685 net.cpp:368] pool5 -> pool5
I0406 18:08:02.255830 86685 net.cpp:120] Setting up pool5
I0406 18:08:02.255827 86686 net.cpp:90] Creating Layer pool5
I0406 18:08:02.255831 86686 net.cpp:410] pool5 <- conv5/bn
I0406 18:08:02.255838 86686 net.cpp:368] pool5 -> pool5
I0406 18:08:02.255846 86686 net.cpp:120] Setting up pool5
I0406 18:08:02.256181 86685 net.cpp:127] Top shape: 16 384 6 6 (221184)
I0406 18:08:02.256191 86685 layer_factory.hpp:74] Creating layer fc6
I0406 18:08:02.256237 86685 net.cpp:90] Creating Layer fc6
I0406 18:08:02.256240 86685 net.cpp:410] fc6 <- pool5
I0406 18:08:02.256247 86685 net.cpp:368] fc6 -> fc6
I0406 18:08:02.256253 86685 net.cpp:120] Setting up fc6
I0406 18:08:02.256270 86686 net.cpp:127] Top shape: 16 384 6 6 (221184)
I0406 18:08:02.256283 86686 layer_factory.hpp:74] Creating layer fc6
I0406 18:08:02.256294 86686 net.cpp:90] Creating Layer fc6
I0406 18:08:02.256299 86686 net.cpp:410] fc6 <- pool5
I0406 18:08:02.256307 86686 net.cpp:368] fc6 -> fc6
I0406 18:08:02.256316 86686 net.cpp:120] Setting up fc6
I0406 18:08:03.816103 86686 net.cpp:127] Top shape: 16 4096 (65536)
I0406 18:08:03.816159 86686 layer_factory.hpp:74] Creating layer relu6
I0406 18:08:03.816177 86686 net.cpp:90] Creating Layer relu6
I0406 18:08:03.816189 86686 net.cpp:410] relu6 <- fc6
I0406 18:08:03.816197 86686 net.cpp:357] relu6 -> fc6 (in-place)
I0406 18:08:03.816217 86686 net.cpp:120] Setting up relu6
I0406 18:08:03.816764 86686 net.cpp:127] Top shape: 16 4096 (65536)
I0406 18:08:03.816774 86686 layer_factory.hpp:74] Creating layer drop6
I0406 18:08:03.816797 86686 net.cpp:90] Creating Layer drop6
I0406 18:08:03.816800 86686 net.cpp:410] drop6 <- fc6
I0406 18:08:03.816805 86686 net.cpp:357] drop6 -> fc6 (in-place)
I0406 18:08:03.816823 86686 net.cpp:120] Setting up drop6
I0406 18:08:03.816829 86686 net.cpp:127] Top shape: 16 4096 (65536)
I0406 18:08:03.816851 86686 layer_factory.hpp:74] Creating layer reshape_data
I0406 18:08:03.816860 86686 net.cpp:90] Creating Layer reshape_data
I0406 18:08:03.816869 86686 net.cpp:410] reshape_data <- fc6
I0406 18:08:03.816887 86686 net.cpp:368] reshape_data -> data_reshape
I0406 18:08:03.816900 86686 net.cpp:120] Setting up reshape_data
I0406 18:08:03.816907 86686 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.816911 86686 layer_factory.hpp:74] Creating layer data_reshape_reshape_data_0_split
I0406 18:08:03.816917 86686 net.cpp:90] Creating Layer data_reshape_reshape_data_0_split
I0406 18:08:03.816932 86686 net.cpp:410] data_reshape_reshape_data_0_split <- data_reshape
I0406 18:08:03.816939 86686 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_0
I0406 18:08:03.816946 86686 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_1
I0406 18:08:03.816963 86686 net.cpp:120] Setting up data_reshape_reshape_data_0_split
I0406 18:08:03.816970 86686 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.816974 86686 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.817008 86686 layer_factory.hpp:74] Creating layer lstm
I0406 18:08:03.817021 86686 net.cpp:90] Creating Layer lstm
I0406 18:08:03.817037 86686 net.cpp:410] lstm <- data_reshape_reshape_data_0_split_0
I0406 18:08:03.817056 86686 net.cpp:410] lstm <- clip_marker_data_2_split_0
I0406 18:08:03.817067 86686 net.cpp:368] lstm -> lstm
I0406 18:08:03.817085 86686 net.cpp:120] Setting up lstm
I0406 18:08:03.817114 86686 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:03.817131 86686 layer_factory.hpp:74] Creating layer lstm_drop
I0406 18:08:03.817140 86686 net.cpp:90] Creating Layer lstm_drop
I0406 18:08:03.817144 86686 net.cpp:410] lstm_drop <- lstm
I0406 18:08:03.817152 86686 net.cpp:368] lstm_drop -> lstm_drop
I0406 18:08:03.817157 86686 net.cpp:120] Setting up lstm_drop
I0406 18:08:03.817163 86686 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:03.817167 86686 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:03.817173 86686 net.cpp:90] Creating Layer reverse
I0406 18:08:03.817178 86686 net.cpp:410] reverse <- clip_marker_data_2_split_1
I0406 18:08:03.817183 86686 net.cpp:410] reverse <- data_reshape_reshape_data_0_split_1
I0406 18:08:03.817188 86686 net.cpp:368] reverse -> inv_lstm_input
I0406 18:08:03.817193 86686 net.cpp:120] Setting up reverse
I0406 18:08:03.817209 86686 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.817211 86686 layer_factory.hpp:74] Creating layer invlstm
I0406 18:08:03.817221 86686 net.cpp:90] Creating Layer invlstm
I0406 18:08:03.817226 86686 net.cpp:410] invlstm <- inv_lstm_input
I0406 18:08:03.817230 86686 net.cpp:410] invlstm <- clip_marker_data_2_split_2
I0406 18:08:03.817237 86686 net.cpp:368] invlstm -> inv_lstm
I0406 18:08:03.817245 86686 net.cpp:120] Setting up invlstm
I0406 18:08:03.817257 86686 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:03.817265 86686 layer_factory.hpp:74] Creating layer inv_lstm_drop
I0406 18:08:03.817277 86686 net.cpp:90] Creating Layer inv_lstm_drop
I0406 18:08:03.817282 86686 net.cpp:410] inv_lstm_drop <- inv_lstm
I0406 18:08:03.817287 86686 net.cpp:368] inv_lstm_drop -> inv_lstm_drop
I0406 18:08:03.817292 86686 net.cpp:120] Setting up inv_lstm_drop
I0406 18:08:03.817298 86686 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:03.817313 86686 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:03.817322 86686 net.cpp:90] Creating Layer reverse
I0406 18:08:03.817327 86686 net.cpp:410] reverse <- clip_marker_data_2_split_3
I0406 18:08:03.817332 86686 net.cpp:410] reverse <- inv_lstm_drop
I0406 18:08:03.817337 86686 net.cpp:368] reverse -> inv_lstm_output
I0406 18:08:03.817342 86686 net.cpp:120] Setting up reverse
I0406 18:08:03.817348 86686 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:03.817351 86686 layer_factory.hpp:74] Creating layer concat
I0406 18:08:03.817360 86686 net.cpp:90] Creating Layer concat
I0406 18:08:03.817364 86686 net.cpp:410] concat <- lstm_drop
I0406 18:08:03.817369 86686 net.cpp:410] concat <- inv_lstm_output
I0406 18:08:03.817374 86686 net.cpp:368] concat -> lstm_concat
I0406 18:08:03.817379 86686 net.cpp:120] Setting up concat
I0406 18:08:03.817396 86686 net.cpp:127] Top shape: 16 1 1024 (16384)
I0406 18:08:03.817400 86686 layer_factory.hpp:74] Creating layer fc8_final
I0406 18:08:03.817411 86686 net.cpp:90] Creating Layer fc8_final
I0406 18:08:03.817416 86686 net.cpp:410] fc8_final <- lstm_concat
I0406 18:08:03.817421 86686 net.cpp:368] fc8_final -> fc8_final
I0406 18:08:03.817428 86686 net.cpp:120] Setting up fc8_final
I0406 18:08:03.817832 86686 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:03.817868 86686 layer_factory.hpp:74] Creating layer fc8_final_fc8_final_0_split
I0406 18:08:03.817874 86686 net.cpp:90] Creating Layer fc8_final_fc8_final_0_split
I0406 18:08:03.817879 86686 net.cpp:410] fc8_final_fc8_final_0_split <- fc8_final
I0406 18:08:03.817884 86686 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_0
I0406 18:08:03.817894 86686 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_1
I0406 18:08:03.817901 86686 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_2
I0406 18:08:03.817919 86686 net.cpp:120] Setting up fc8_final_fc8_final_0_split
I0406 18:08:03.817927 86686 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:03.817931 86686 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:03.817936 86686 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:03.817951 86686 layer_factory.hpp:74] Creating layer loss
I0406 18:08:03.817960 86686 net.cpp:90] Creating Layer loss
I0406 18:08:03.817965 86686 net.cpp:410] loss <- fc8_final_fc8_final_0_split_0
I0406 18:08:03.817968 86686 net.cpp:410] loss <- label_data_1_split_0
I0406 18:08:03.817988 86686 net.cpp:368] loss -> loss
I0406 18:08:03.817996 86686 net.cpp:120] Setting up loss
I0406 18:08:03.818001 86686 layer_factory.hpp:74] Creating layer loss
I0406 18:08:03.818485 86686 net.cpp:127] Top shape: (1)
I0406 18:08:03.818496 86686 net.cpp:129]     with loss weight 1
I0406 18:08:03.818507 86686 layer_factory.hpp:74] Creating layer accuracy
I0406 18:08:03.818526 86686 net.cpp:90] Creating Layer accuracy
I0406 18:08:03.818531 86686 net.cpp:410] accuracy <- fc8_final_fc8_final_0_split_1
I0406 18:08:03.818536 86686 net.cpp:410] accuracy <- label_data_1_split_1
I0406 18:08:03.818559 86686 net.cpp:368] accuracy -> accuracy
I0406 18:08:03.818567 86686 net.cpp:120] Setting up accuracy
I0406 18:08:03.818572 86686 net.cpp:127] Top shape: (1)
I0406 18:08:03.818577 86686 layer_factory.hpp:74] Creating layer accuracy_top2
I0406 18:08:03.818583 86686 net.cpp:90] Creating Layer accuracy_top2
I0406 18:08:03.818585 86686 net.cpp:410] accuracy_top2 <- fc8_final_fc8_final_0_split_2
I0406 18:08:03.818590 86686 net.cpp:410] accuracy_top2 <- label_data_1_split_2
I0406 18:08:03.818595 86686 net.cpp:368] accuracy_top2 -> accuracy_top2
I0406 18:08:03.818600 86686 net.cpp:120] Setting up accuracy_top2
I0406 18:08:03.818604 86686 net.cpp:127] Top shape: (1)
I0406 18:08:03.818608 86686 net.cpp:194] accuracy_top2 does not need backward computation.
I0406 18:08:03.818613 86686 net.cpp:194] accuracy does not need backward computation.
I0406 18:08:03.818616 86686 net.cpp:192] loss needs backward computation.
I0406 18:08:03.818620 86686 net.cpp:192] fc8_final_fc8_final_0_split needs backward computation.
I0406 18:08:03.818635 86686 net.cpp:192] fc8_final needs backward computation.
I0406 18:08:03.818639 86686 net.cpp:192] concat needs backward computation.
I0406 18:08:03.818642 86686 net.cpp:192] reverse needs backward computation.
I0406 18:08:03.818647 86686 net.cpp:192] inv_lstm_drop needs backward computation.
I0406 18:08:03.818650 86686 net.cpp:192] invlstm needs backward computation.
I0406 18:08:03.818655 86686 net.cpp:192] reverse needs backward computation.
I0406 18:08:03.818658 86686 net.cpp:192] lstm_drop needs backward computation.
I0406 18:08:03.818662 86686 net.cpp:192] lstm needs backward computation.
I0406 18:08:03.818668 86686 net.cpp:192] data_reshape_reshape_data_0_split needs backward computation.
I0406 18:08:03.818673 86686 net.cpp:192] reshape_data needs backward computation.
I0406 18:08:03.818677 86686 net.cpp:192] drop6 needs backward computation.
I0406 18:08:03.818681 86686 net.cpp:192] relu6 needs backward computation.
I0406 18:08:03.818683 86686 net.cpp:192] fc6 needs backward computation.
I0406 18:08:03.818687 86686 net.cpp:192] pool5 needs backward computation.
I0406 18:08:03.818693 86686 net.cpp:192] relu5 needs backward computation.
I0406 18:08:03.818697 86686 net.cpp:192] bn5 needs backward computation.
I0406 18:08:03.818702 86686 net.cpp:192] conv5 needs backward computation.
I0406 18:08:03.818708 86686 net.cpp:192] relu4 needs backward computation.
I0406 18:08:03.818713 86686 net.cpp:192] bn4 needs backward computation.
I0406 18:08:03.818718 86686 net.cpp:192] conv4 needs backward computation.
I0406 18:08:03.818722 86686 net.cpp:192] relu3 needs backward computation.
I0406 18:08:03.818727 86686 net.cpp:192] bn3 needs backward computation.
I0406 18:08:03.818730 86686 net.cpp:192] conv3 needs backward computation.
I0406 18:08:03.818734 86686 net.cpp:192] norm2 needs backward computation.
I0406 18:08:03.818750 86686 net.cpp:192] pool2 needs backward computation.
I0406 18:08:03.818755 86686 net.cpp:192] relu2 needs backward computation.
I0406 18:08:03.818759 86686 net.cpp:192] bn2 needs backward computation.
I0406 18:08:03.818763 86686 net.cpp:192] conv2 needs backward computation.
I0406 18:08:03.818768 86686 net.cpp:192] norm1 needs backward computation.
I0406 18:08:03.818771 86686 net.cpp:192] pool1 needs backward computation.
I0406 18:08:03.818776 86686 net.cpp:192] relu1 needs backward computation.
I0406 18:08:03.818780 86686 net.cpp:192] bn1 needs backward computation.
I0406 18:08:03.818783 86686 net.cpp:192] conv1 needs backward computation.
I0406 18:08:03.818789 86686 net.cpp:194] clip_marker_data_2_split does not need backward computation.
I0406 18:08:03.818794 86686 net.cpp:194] label_data_1_split does not need backward computation.
I0406 18:08:03.818797 86686 net.cpp:194] data does not need backward computation.
I0406 18:08:03.818800 86686 net.cpp:235] This network produces output accuracy
I0406 18:08:03.818804 86686 net.cpp:235] This network produces output accuracy_top2
I0406 18:08:03.818809 86686 net.cpp:235] This network produces output loss
I0406 18:08:03.818841 86686 net.cpp:482] Collecting Learning Rate and Weight Decay.
I0406 18:08:03.818851 86686 net.cpp:247] Network initialization done.
I0406 18:08:03.818855 86686 net.cpp:248] Memory required for data: 380951820
I0406 18:08:03.819097 86686 solver.cpp:58] Solver scaffolding done.
I0406 18:08:03.819213 86686 caffe.cpp:94] Finetuning from rgb.caffemodel
I0406 18:08:03.828330 86685 net.cpp:127] Top shape: 16 4096 (65536)
I0406 18:08:03.828377 86685 layer_factory.hpp:74] Creating layer relu6
I0406 18:08:03.828393 86685 net.cpp:90] Creating Layer relu6
I0406 18:08:03.828402 86685 net.cpp:410] relu6 <- fc6
I0406 18:08:03.828423 86685 net.cpp:357] relu6 -> fc6 (in-place)
I0406 18:08:03.828436 86685 net.cpp:120] Setting up relu6
I0406 18:08:03.828987 86685 net.cpp:127] Top shape: 16 4096 (65536)
I0406 18:08:03.828997 86685 layer_factory.hpp:74] Creating layer drop6
I0406 18:08:03.829020 86685 net.cpp:90] Creating Layer drop6
I0406 18:08:03.829035 86685 net.cpp:410] drop6 <- fc6
I0406 18:08:03.829042 86685 net.cpp:357] drop6 -> fc6 (in-place)
I0406 18:08:03.829048 86685 net.cpp:120] Setting up drop6
I0406 18:08:03.829054 86685 net.cpp:127] Top shape: 16 4096 (65536)
I0406 18:08:03.829058 86685 layer_factory.hpp:74] Creating layer reshape_data
I0406 18:08:03.829078 86685 net.cpp:90] Creating Layer reshape_data
I0406 18:08:03.829082 86685 net.cpp:410] reshape_data <- fc6
I0406 18:08:03.829102 86685 net.cpp:368] reshape_data -> data_reshape
I0406 18:08:03.829109 86685 net.cpp:120] Setting up reshape_data
I0406 18:08:03.829116 86685 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.829128 86685 layer_factory.hpp:74] Creating layer data_reshape_reshape_data_0_split
I0406 18:08:03.829134 86685 net.cpp:90] Creating Layer data_reshape_reshape_data_0_split
I0406 18:08:03.829138 86685 net.cpp:410] data_reshape_reshape_data_0_split <- data_reshape
I0406 18:08:03.829144 86685 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_0
I0406 18:08:03.829152 86685 net.cpp:368] data_reshape_reshape_data_0_split -> data_reshape_reshape_data_0_split_1
I0406 18:08:03.829162 86685 net.cpp:120] Setting up data_reshape_reshape_data_0_split
I0406 18:08:03.829169 86685 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.829174 86685 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:03.829203 86685 layer_factory.hpp:74] Creating layer lstm
I0406 18:08:03.829216 86685 net.cpp:90] Creating Layer lstm
I0406 18:08:03.829221 86685 net.cpp:410] lstm <- data_reshape_reshape_data_0_split_0
I0406 18:08:03.829226 86685 net.cpp:410] lstm <- clip_marker_data_2_split_0
I0406 18:08:03.829247 86685 net.cpp:368] lstm -> lstm
I0406 18:08:03.829255 86685 net.cpp:120] Setting up lstm
I0406 18:08:04.090374 86685 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:04.090425 86685 layer_factory.hpp:74] Creating layer lstm_drop
I0406 18:08:04.090435 86685 net.cpp:90] Creating Layer lstm_drop
I0406 18:08:04.090441 86685 net.cpp:410] lstm_drop <- lstm
I0406 18:08:04.090450 86685 net.cpp:368] lstm_drop -> lstm_drop
I0406 18:08:04.090461 86685 net.cpp:120] Setting up lstm_drop
I0406 18:08:04.090468 86685 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:04.090483 86685 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:04.090492 86685 net.cpp:90] Creating Layer reverse
I0406 18:08:04.090504 86685 net.cpp:410] reverse <- clip_marker_data_2_split_1
I0406 18:08:04.090510 86685 net.cpp:410] reverse <- data_reshape_reshape_data_0_split_1
I0406 18:08:04.090528 86685 net.cpp:368] reverse -> inv_lstm_input
I0406 18:08:04.090533 86685 net.cpp:120] Setting up reverse
I0406 18:08:04.090539 86685 net.cpp:127] Top shape: 16 1 4096 (65536)
I0406 18:08:04.090543 86685 layer_factory.hpp:74] Creating layer invlstm
I0406 18:08:04.090555 86685 net.cpp:90] Creating Layer invlstm
I0406 18:08:04.090560 86685 net.cpp:410] invlstm <- inv_lstm_input
I0406 18:08:04.090565 86685 net.cpp:410] invlstm <- clip_marker_data_2_split_2
I0406 18:08:04.090574 86685 net.cpp:368] invlstm -> inv_lstm
I0406 18:08:04.090582 86685 net.cpp:120] Setting up invlstm
E0406 18:08:04.306231 86686 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated V0LayerParameter: rgb.caffemodel
I0406 18:08:04.349462 86685 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:04.349510 86685 layer_factory.hpp:74] Creating layer inv_lstm_drop
I0406 18:08:04.349526 86685 net.cpp:90] Creating Layer inv_lstm_drop
I0406 18:08:04.349532 86685 net.cpp:410] inv_lstm_drop <- inv_lstm
I0406 18:08:04.349540 86685 net.cpp:368] inv_lstm_drop -> inv_lstm_drop
I0406 18:08:04.349552 86685 net.cpp:120] Setting up inv_lstm_drop
I0406 18:08:04.349560 86685 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:04.349563 86685 layer_factory.hpp:74] Creating layer reverse
I0406 18:08:04.349584 86685 net.cpp:90] Creating Layer reverse
I0406 18:08:04.349596 86685 net.cpp:410] reverse <- clip_marker_data_2_split_3
I0406 18:08:04.349601 86685 net.cpp:410] reverse <- inv_lstm_drop
I0406 18:08:04.349606 86685 net.cpp:368] reverse -> inv_lstm_output
I0406 18:08:04.349612 86685 net.cpp:120] Setting up reverse
I0406 18:08:04.349629 86685 net.cpp:127] Top shape: 16 1 512 (8192)
I0406 18:08:04.349633 86685 layer_factory.hpp:74] Creating layer concat
I0406 18:08:04.349644 86685 net.cpp:90] Creating Layer concat
I0406 18:08:04.349650 86685 net.cpp:410] concat <- lstm_drop
I0406 18:08:04.349654 86685 net.cpp:410] concat <- inv_lstm_output
I0406 18:08:04.349660 86685 net.cpp:368] concat -> lstm_concat
I0406 18:08:04.349666 86685 net.cpp:120] Setting up concat
I0406 18:08:04.349673 86685 net.cpp:127] Top shape: 16 1 1024 (16384)
I0406 18:08:04.349676 86685 layer_factory.hpp:74] Creating layer fc8_final
I0406 18:08:04.349686 86685 net.cpp:90] Creating Layer fc8_final
I0406 18:08:04.349691 86685 net.cpp:410] fc8_final <- lstm_concat
I0406 18:08:04.349697 86685 net.cpp:368] fc8_final -> fc8_final
I0406 18:08:04.349704 86685 net.cpp:120] Setting up fc8_final
I0406 18:08:04.350114 86685 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:04.350123 86685 layer_factory.hpp:74] Creating layer fc8_final_fc8_final_0_split
I0406 18:08:04.350142 86685 net.cpp:90] Creating Layer fc8_final_fc8_final_0_split
I0406 18:08:04.350147 86685 net.cpp:410] fc8_final_fc8_final_0_split <- fc8_final
I0406 18:08:04.350152 86685 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_0
I0406 18:08:04.350162 86685 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_1
I0406 18:08:04.350179 86685 net.cpp:368] fc8_final_fc8_final_0_split -> fc8_final_fc8_final_0_split_2
I0406 18:08:04.350201 86685 net.cpp:120] Setting up fc8_final_fc8_final_0_split
I0406 18:08:04.350209 86685 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:04.350214 86685 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:04.350217 86685 net.cpp:127] Top shape: 16 1 12 (192)
I0406 18:08:04.350220 86685 layer_factory.hpp:74] Creating layer loss
I0406 18:08:04.350230 86685 net.cpp:90] Creating Layer loss
I0406 18:08:04.350234 86685 net.cpp:410] loss <- fc8_final_fc8_final_0_split_0
I0406 18:08:04.350239 86685 net.cpp:410] loss <- label_data_1_split_0
I0406 18:08:04.350245 86685 net.cpp:368] loss -> loss
I0406 18:08:04.350252 86685 net.cpp:120] Setting up loss
I0406 18:08:04.350258 86685 layer_factory.hpp:74] Creating layer loss
I0406 18:08:04.350843 86685 net.cpp:127] Top shape: (1)
I0406 18:08:04.350854 86685 net.cpp:129]     with loss weight 1
I0406 18:08:04.350878 86685 layer_factory.hpp:74] Creating layer accuracy
I0406 18:08:04.350898 86685 net.cpp:90] Creating Layer accuracy
I0406 18:08:04.350901 86685 net.cpp:410] accuracy <- fc8_final_fc8_final_0_split_1
I0406 18:08:04.350905 86685 net.cpp:410] accuracy <- label_data_1_split_1
I0406 18:08:04.350914 86685 net.cpp:368] accuracy -> accuracy
I0406 18:08:04.350931 86685 net.cpp:120] Setting up accuracy
I0406 18:08:04.350937 86685 net.cpp:127] Top shape: (1)
I0406 18:08:04.350941 86685 layer_factory.hpp:74] Creating layer accuracy_top2
I0406 18:08:04.350950 86685 net.cpp:90] Creating Layer accuracy_top2
I0406 18:08:04.350960 86685 net.cpp:410] accuracy_top2 <- fc8_final_fc8_final_0_split_2
I0406 18:08:04.350963 86685 net.cpp:410] accuracy_top2 <- label_data_1_split_2
I0406 18:08:04.350968 86685 net.cpp:368] accuracy_top2 -> accuracy_top2
I0406 18:08:04.350986 86685 net.cpp:120] Setting up accuracy_top2
I0406 18:08:04.350989 86685 net.cpp:127] Top shape: (1)
I0406 18:08:04.350993 86685 net.cpp:194] accuracy_top2 does not need backward computation.
I0406 18:08:04.350997 86685 net.cpp:194] accuracy does not need backward computation.
I0406 18:08:04.351001 86685 net.cpp:192] loss needs backward computation.
I0406 18:08:04.351004 86685 net.cpp:192] fc8_final_fc8_final_0_split needs backward computation.
I0406 18:08:04.351008 86685 net.cpp:192] fc8_final needs backward computation.
I0406 18:08:04.351011 86685 net.cpp:192] concat needs backward computation.
I0406 18:08:04.351016 86685 net.cpp:192] reverse needs backward computation.
I0406 18:08:04.351021 86685 net.cpp:192] inv_lstm_drop needs backward computation.
I0406 18:08:04.351023 86685 net.cpp:192] invlstm needs backward computation.
I0406 18:08:04.351028 86685 net.cpp:192] reverse needs backward computation.
I0406 18:08:04.351034 86685 net.cpp:192] lstm_drop needs backward computation.
I0406 18:08:04.351038 86685 net.cpp:192] lstm needs backward computation.
I0406 18:08:04.351042 86685 net.cpp:192] data_reshape_reshape_data_0_split needs backward computation.
I0406 18:08:04.351047 86685 net.cpp:192] reshape_data needs backward computation.
I0406 18:08:04.351050 86685 net.cpp:192] drop6 needs backward computation.
I0406 18:08:04.351053 86685 net.cpp:192] relu6 needs backward computation.
I0406 18:08:04.351058 86685 net.cpp:192] fc6 needs backward computation.
I0406 18:08:04.351061 86685 net.cpp:192] pool5 needs backward computation.
I0406 18:08:04.351066 86685 net.cpp:192] relu5 needs backward computation.
I0406 18:08:04.351070 86685 net.cpp:192] bn5 needs backward computation.
I0406 18:08:04.351075 86685 net.cpp:192] conv5 needs backward computation.
I0406 18:08:04.351079 86685 net.cpp:192] relu4 needs backward computation.
I0406 18:08:04.351084 86685 net.cpp:192] bn4 needs backward computation.
I0406 18:08:04.351089 86685 net.cpp:192] conv4 needs backward computation.
I0406 18:08:04.351094 86685 net.cpp:192] relu3 needs backward computation.
I0406 18:08:04.351099 86685 net.cpp:192] bn3 needs backward computation.
I0406 18:08:04.351102 86685 net.cpp:192] conv3 needs backward computation.
I0406 18:08:04.351106 86685 net.cpp:192] norm2 needs backward computation.
I0406 18:08:04.351124 86685 net.cpp:192] pool2 needs backward computation.
I0406 18:08:04.351128 86685 net.cpp:192] relu2 needs backward computation.
I0406 18:08:04.351133 86685 net.cpp:192] bn2 needs backward computation.
I0406 18:08:04.351136 86685 net.cpp:192] conv2 needs backward computation.
I0406 18:08:04.351140 86685 net.cpp:192] norm1 needs backward computation.
I0406 18:08:04.351145 86685 net.cpp:192] pool1 needs backward computation.
I0406 18:08:04.351150 86685 net.cpp:192] relu1 needs backward computation.
I0406 18:08:04.351153 86685 net.cpp:192] bn1 needs backward computation.
I0406 18:08:04.351157 86685 net.cpp:192] conv1 needs backward computation.
I0406 18:08:04.351162 86685 net.cpp:194] clip_marker_data_2_split does not need backward computation.
I0406 18:08:04.351167 86685 net.cpp:194] label_data_1_split does not need backward computation.
I0406 18:08:04.351171 86685 net.cpp:194] data does not need backward computation.
I0406 18:08:04.351174 86685 net.cpp:235] This network produces output accuracy
I0406 18:08:04.351178 86685 net.cpp:235] This network produces output accuracy_top2
I0406 18:08:04.351181 86685 net.cpp:235] This network produces output loss
I0406 18:08:04.351212 86685 net.cpp:482] Collecting Learning Rate and Weight Decay.
I0406 18:08:04.351224 86685 net.cpp:247] Network initialization done.
I0406 18:08:04.351228 86685 net.cpp:248] Memory required for data: 380951820
I0406 18:08:04.351512 86685 solver.cpp:58] Solver scaffolding done.
I0406 18:08:04.351655 86685 caffe.cpp:94] Finetuning from rgb.caffemodel
I0406 18:08:04.755025 86686 upgrade_proto.cpp:617] Successfully upgraded file specified using deprecated V0LayerParameter
E0406 18:08:04.755054 86686 upgrade_proto.cpp:620] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text for prototxt and ./build/tools/upgrade_net_proto_binary for model weights upgrade this and any other net protos to the new format.
E0406 18:08:04.756381 86686 upgrade_proto.cpp:636] Attempting to upgrade input file specified using deprecated V1LayerParameter: rgb.caffemodel
E0406 18:08:04.918823 86685 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated V0LayerParameter: rgb.caffemodel
I0406 18:08:05.033754 86686 upgrade_proto.cpp:644] Successfully upgraded file specified using deprecated V1LayerParameter
I0406 18:08:05.035228 86686 net.cpp:733] Ignoring source layer data1
I0406 18:08:05.035238 86686 net.cpp:733] Ignoring source layer data2
I0406 18:08:05.035254 86686 net.cpp:736] Copying source layer data
I0406 18:08:05.035257 86686 net.cpp:733] Ignoring source layer label
I0406 18:08:05.035260 86686 net.cpp:736] Copying source layer conv1
I0406 18:08:05.035301 86686 net.cpp:736] Copying source layer relu1
I0406 18:08:05.035308 86686 net.cpp:736] Copying source layer pool1
I0406 18:08:05.035323 86686 net.cpp:736] Copying source layer norm1
I0406 18:08:05.035327 86686 net.cpp:736] Copying source layer conv2
I0406 18:08:05.035974 86686 net.cpp:736] Copying source layer relu2
I0406 18:08:05.035981 86686 net.cpp:736] Copying source layer pool2
I0406 18:08:05.035997 86686 net.cpp:736] Copying source layer norm2
I0406 18:08:05.036001 86686 net.cpp:736] Copying source layer conv3
I0406 18:08:05.037581 86686 net.cpp:736] Copying source layer relu3
I0406 18:08:05.037593 86686 net.cpp:736] Copying source layer conv4
I0406 18:08:05.038852 86686 net.cpp:736] Copying source layer relu4
I0406 18:08:05.038863 86686 net.cpp:736] Copying source layer conv5
I0406 18:08:05.039770 86686 net.cpp:736] Copying source layer relu5
I0406 18:08:05.039779 86686 net.cpp:736] Copying source layer pool5
I0406 18:08:05.039795 86686 net.cpp:736] Copying source layer fc6
I0406 18:08:05.078958 86686 net.cpp:736] Copying source layer relu6
I0406 18:08:05.078999 86686 net.cpp:736] Copying source layer drop6
I0406 18:08:05.079005 86686 net.cpp:733] Ignoring source layer fc7
I0406 18:08:05.079008 86686 net.cpp:733] Ignoring source layer relu7
I0406 18:08:05.079011 86686 net.cpp:733] Ignoring source layer drop7
I0406 18:08:05.079038 86686 net.cpp:733] Ignoring source layer fc8
I0406 18:08:05.079042 86686 net.cpp:736] Copying source layer loss
I0406 18:08:05.311241 86685 upgrade_proto.cpp:617] Successfully upgraded file specified using deprecated V0LayerParameter
E0406 18:08:05.311285 86685 upgrade_proto.cpp:620] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text for prototxt and ./build/tools/upgrade_net_proto_binary for model weights upgrade this and any other net protos to the new format.
E0406 18:08:05.312775 86685 upgrade_proto.cpp:636] Attempting to upgrade input file specified using deprecated V1LayerParameter: rgb.caffemodel
I0406 18:08:05.593665 86685 upgrade_proto.cpp:644] Successfully upgraded file specified using deprecated V1LayerParameter
I0406 18:08:05.595120 86685 net.cpp:733] Ignoring source layer data1
I0406 18:08:05.595130 86685 net.cpp:733] Ignoring source layer data2
I0406 18:08:05.595145 86685 net.cpp:736] Copying source layer data
I0406 18:08:05.595149 86685 net.cpp:733] Ignoring source layer label
I0406 18:08:05.595152 86685 net.cpp:736] Copying source layer conv1
I0406 18:08:05.595193 86685 net.cpp:736] Copying source layer relu1
I0406 18:08:05.595199 86685 net.cpp:736] Copying source layer pool1
I0406 18:08:05.595202 86685 net.cpp:736] Copying source layer norm1
I0406 18:08:05.595206 86685 net.cpp:736] Copying source layer conv2
I0406 18:08:05.595819 86685 net.cpp:736] Copying source layer relu2
I0406 18:08:05.595839 86685 net.cpp:736] Copying source layer pool2
I0406 18:08:05.595842 86685 net.cpp:736] Copying source layer norm2
I0406 18:08:05.595857 86685 net.cpp:736] Copying source layer conv3
I0406 18:08:05.597622 86685 net.cpp:736] Copying source layer relu3
I0406 18:08:05.597631 86685 net.cpp:736] Copying source layer conv4
E0406 18:08:05.598449 86686 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated V0LayerParameter: rgb.caffemodel
I0406 18:08:05.599045 86685 net.cpp:736] Copying source layer relu4
I0406 18:08:05.599066 86685 net.cpp:736] Copying source layer conv5
I0406 18:08:05.599988 86685 net.cpp:736] Copying source layer relu5
I0406 18:08:05.599997 86685 net.cpp:736] Copying source layer pool5
I0406 18:08:05.600013 86685 net.cpp:736] Copying source layer fc6
I0406 18:08:05.638638 86685 net.cpp:736] Copying source layer relu6
I0406 18:08:05.638659 86685 net.cpp:736] Copying source layer drop6
I0406 18:08:05.638674 86685 net.cpp:733] Ignoring source layer fc7
I0406 18:08:05.638679 86685 net.cpp:733] Ignoring source layer relu7
I0406 18:08:05.638681 86685 net.cpp:733] Ignoring source layer drop7
I0406 18:08:05.638711 86685 net.cpp:733] Ignoring source layer fc8
I0406 18:08:05.638715 86685 net.cpp:736] Copying source layer loss
I0406 18:08:06.011064 86686 upgrade_proto.cpp:617] Successfully upgraded file specified using deprecated V0LayerParameter
E0406 18:08:06.011093 86686 upgrade_proto.cpp:620] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text for prototxt and ./build/tools/upgrade_net_proto_binary for model weights upgrade this and any other net protos to the new format.
E0406 18:08:06.012755 86686 upgrade_proto.cpp:636] Attempting to upgrade input file specified using deprecated V1LayerParameter: rgb.caffemodel
E0406 18:08:06.186331 86685 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated V0LayerParameter: rgb.caffemodel
I0406 18:08:06.286650 86686 upgrade_proto.cpp:644] Successfully upgraded file specified using deprecated V1LayerParameter
I0406 18:08:06.288046 86686 net.cpp:733] Ignoring source layer data1
I0406 18:08:06.288058 86686 net.cpp:733] Ignoring source layer data2
I0406 18:08:06.288072 86686 net.cpp:736] Copying source layer data
I0406 18:08:06.288075 86686 net.cpp:733] Ignoring source layer label
I0406 18:08:06.288079 86686 net.cpp:736] Copying source layer conv1
I0406 18:08:06.288118 86686 net.cpp:736] Copying source layer relu1
I0406 18:08:06.288125 86686 net.cpp:736] Copying source layer pool1
I0406 18:08:06.288127 86686 net.cpp:736] Copying source layer norm1
I0406 18:08:06.288131 86686 net.cpp:736] Copying source layer conv2
I0406 18:08:06.288733 86686 net.cpp:736] Copying source layer relu2
I0406 18:08:06.288739 86686 net.cpp:736] Copying source layer pool2
I0406 18:08:06.288743 86686 net.cpp:736] Copying source layer norm2
I0406 18:08:06.288758 86686 net.cpp:736] Copying source layer conv3
I0406 18:08:06.290336 86686 net.cpp:736] Copying source layer relu3
I0406 18:08:06.290350 86686 net.cpp:736] Copying source layer conv4
I0406 18:08:06.291595 86686 net.cpp:736] Copying source layer relu4
I0406 18:08:06.291604 86686 net.cpp:736] Copying source layer conv5
I0406 18:08:06.292512 86686 net.cpp:736] Copying source layer relu5
I0406 18:08:06.292520 86686 net.cpp:736] Copying source layer pool5
I0406 18:08:06.292534 86686 net.cpp:736] Copying source layer fc6
I0406 18:08:06.331149 86686 net.cpp:736] Copying source layer relu6
I0406 18:08:06.331173 86686 net.cpp:736] Copying source layer drop6
I0406 18:08:06.331189 86686 net.cpp:733] Ignoring source layer fc7
I0406 18:08:06.331192 86686 net.cpp:733] Ignoring source layer relu7
I0406 18:08:06.331197 86686 net.cpp:733] Ignoring source layer drop7
I0406 18:08:06.331199 86686 net.cpp:733] Ignoring source layer fc8
I0406 18:08:06.331203 86686 net.cpp:736] Copying source layer loss
I0406 18:08:06.337576 86686 solver.cpp:304] Solving lstm_joints
I0406 18:08:06.337605 86686 solver.cpp:305] Learning Rate Policy: step
I0406 18:08:06.565372 86685 upgrade_proto.cpp:617] Successfully upgraded file specified using deprecated V0LayerParameter
E0406 18:08:06.565402 86685 upgrade_proto.cpp:620] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text for prototxt and ./build/tools/upgrade_net_proto_binary for model weights upgrade this and any other net protos to the new format.
E0406 18:08:06.566911 86685 upgrade_proto.cpp:636] Attempting to upgrade input file specified using deprecated V1LayerParameter: rgb.caffemodel
I0406 18:08:06.826525 86685 upgrade_proto.cpp:644] Successfully upgraded file specified using deprecated V1LayerParameter
I0406 18:08:06.828001 86685 net.cpp:733] Ignoring source layer data1
I0406 18:08:06.828009 86685 net.cpp:733] Ignoring source layer data2
I0406 18:08:06.828013 86685 net.cpp:736] Copying source layer data
I0406 18:08:06.828028 86685 net.cpp:733] Ignoring source layer label
I0406 18:08:06.828032 86685 net.cpp:736] Copying source layer conv1
I0406 18:08:06.828071 86685 net.cpp:736] Copying source layer relu1
I0406 18:08:06.828076 86685 net.cpp:736] Copying source layer pool1
I0406 18:08:06.828080 86685 net.cpp:736] Copying source layer norm1
I0406 18:08:06.828083 86685 net.cpp:736] Copying source layer conv2
I0406 18:08:06.828696 86685 net.cpp:736] Copying source layer relu2
I0406 18:08:06.828702 86685 net.cpp:736] Copying source layer pool2
I0406 18:08:06.828706 86685 net.cpp:736] Copying source layer norm2
I0406 18:08:06.828722 86685 net.cpp:736] Copying source layer conv3
I0406 18:08:06.830240 86685 net.cpp:736] Copying source layer relu3
I0406 18:08:06.830250 86685 net.cpp:736] Copying source layer conv4
I0406 18:08:06.831496 86685 net.cpp:736] Copying source layer relu4
I0406 18:08:06.831506 86685 net.cpp:736] Copying source layer conv5
I0406 18:08:06.832460 86685 net.cpp:736] Copying source layer relu5
I0406 18:08:06.832468 86685 net.cpp:736] Copying source layer pool5
I0406 18:08:06.832470 86685 net.cpp:736] Copying source layer fc6
I0406 18:08:06.871937 86685 net.cpp:736] Copying source layer relu6
I0406 18:08:06.871960 86685 net.cpp:736] Copying source layer drop6
I0406 18:08:06.871976 86685 net.cpp:733] Ignoring source layer fc7
I0406 18:08:06.871979 86685 net.cpp:733] Ignoring source layer relu7
I0406 18:08:06.871984 86685 net.cpp:733] Ignoring source layer drop7
I0406 18:08:06.871986 86685 net.cpp:733] Ignoring source layer fc8
I0406 18:08:06.871990 86685 net.cpp:736] Copying source layer loss
I0406 18:08:06.878379 86685 solver.cpp:304] Solving lstm_joints
I0406 18:08:06.878399 86685 solver.cpp:305] Learning Rate Policy: step
I0406 18:08:08.262943 86685 solver.cpp:247] Iteration 0, loss = 2.48555
I0406 18:08:08.262990 86685 solver.cpp:263]     Train net output #0: accuracy = 0.0625
I0406 18:08:08.263000 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.125
I0406 18:08:08.263007 86685 solver.cpp:263]     Train net output #2: loss = 2.48555 (* 1 = 2.48555 loss)
I0406 18:08:08.263021 86685 solver.cpp:578] Iteration 0, lr = 0.001
I0406 18:08:32.817771 86685 solver.cpp:247] Iteration 20, loss = 2.48507
I0406 18:08:32.837086 86685 solver.cpp:263]     Train net output #0: accuracy = 0
I0406 18:08:32.837111 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0
I0406 18:08:32.837136 86685 solver.cpp:263]     Train net output #2: loss = 2.48566 (* 1 = 2.48566 loss)
I0406 18:08:32.840858 86685 solver.cpp:578] Iteration 20, lr = 0.001
I0406 18:08:57.449101 86685 solver.cpp:247] Iteration 40, loss = 2.48517
I0406 18:08:57.456676 86685 solver.cpp:263]     Train net output #0: accuracy = 0.0625
I0406 18:08:57.456692 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.125
I0406 18:08:57.456710 86685 solver.cpp:263]     Train net output #2: loss = 2.48547 (* 1 = 2.48547 loss)
I0406 18:08:57.456722 86685 solver.cpp:578] Iteration 40, lr = 0.001
I0406 18:09:22.172405 86685 solver.cpp:247] Iteration 60, loss = 2.48505
I0406 18:09:22.172683 86685 solver.cpp:263]     Train net output #0: accuracy = 0.0625
I0406 18:09:22.172729 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.1875
I0406 18:09:22.172740 86685 solver.cpp:263]     Train net output #2: loss = 2.48441 (* 1 = 2.48441 loss)
I0406 18:09:22.172765 86685 solver.cpp:578] Iteration 60, lr = 0.001
I0406 18:09:46.886481 86685 solver.cpp:247] Iteration 80, loss = 2.48479
I0406 18:09:46.886539 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:09:46.886550 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.3125
I0406 18:09:46.886560 86685 solver.cpp:263]     Train net output #2: loss = 2.48396 (* 1 = 2.48396 loss)
I0406 18:09:46.886569 86685 solver.cpp:578] Iteration 80, lr = 0.001
I0406 18:10:10.446287 86685 solver.cpp:360] Iteration 100, Testing net (#0)
I0406 18:10:16.088296 86685 solver.cpp:431]     Test net output #0: accuracy = 0.1125
I0406 18:10:16.088352 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.175
I0406 18:10:16.088368 86685 solver.cpp:431]     Test net output #2: loss = 2.48481 (* 1 = 2.48481 loss)
I0406 18:10:17.119683 86685 solver.cpp:247] Iteration 100, loss = 2.48467
I0406 18:10:17.119732 86685 solver.cpp:263]     Train net output #0: accuracy = 0.0625
I0406 18:10:17.119743 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.125
I0406 18:10:17.119755 86685 solver.cpp:263]     Train net output #2: loss = 2.4862 (* 1 = 2.4862 loss)
I0406 18:10:17.119762 86685 solver.cpp:578] Iteration 100, lr = 0.001
I0406 18:10:41.681982 86685 solver.cpp:247] Iteration 120, loss = 2.48453
I0406 18:10:41.690277 86685 solver.cpp:263]     Train net output #0: accuracy = 0.125
I0406 18:10:41.690309 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.1875
I0406 18:10:41.690321 86685 solver.cpp:263]     Train net output #2: loss = 2.48303 (* 1 = 2.48303 loss)
I0406 18:10:41.690343 86685 solver.cpp:578] Iteration 120, lr = 0.001
I0406 18:11:06.402221 86685 solver.cpp:247] Iteration 140, loss = 2.48433
I0406 18:11:06.402281 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:11:06.402293 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.1875
I0406 18:11:06.402305 86685 solver.cpp:263]     Train net output #2: loss = 2.48139 (* 1 = 2.48139 loss)
I0406 18:11:06.402317 86685 solver.cpp:578] Iteration 140, lr = 0.001
I0406 18:11:31.025403 86685 solver.cpp:247] Iteration 160, loss = 2.48412
I0406 18:11:31.027714 86685 solver.cpp:263]     Train net output #0: accuracy = 0.0625
I0406 18:11:31.027730 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.3125
I0406 18:11:31.027742 86685 solver.cpp:263]     Train net output #2: loss = 2.48278 (* 1 = 2.48278 loss)
I0406 18:11:31.027752 86685 solver.cpp:578] Iteration 160, lr = 0.001
I0406 18:11:55.769196 86685 solver.cpp:247] Iteration 180, loss = 2.48394
I0406 18:11:55.777729 86685 solver.cpp:263]     Train net output #0: accuracy = 0
I0406 18:11:55.777742 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.0625
I0406 18:11:55.777753 86685 solver.cpp:263]     Train net output #2: loss = 2.48104 (* 1 = 2.48104 loss)
I0406 18:11:55.777760 86685 solver.cpp:578] Iteration 180, lr = 0.001
I0406 18:12:19.206310 86685 solver.cpp:360] Iteration 200, Testing net (#0)
I0406 18:12:24.889672 86685 solver.cpp:431]     Test net output #0: accuracy = 0.15
I0406 18:12:24.889727 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.3
I0406 18:12:24.889742 86685 solver.cpp:431]     Test net output #2: loss = 2.48016 (* 1 = 2.48016 loss)
I0406 18:12:25.706794 86685 solver.cpp:247] Iteration 200, loss = 2.48372
I0406 18:12:25.706856 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:12:25.706869 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.4375
I0406 18:12:25.706881 86685 solver.cpp:263]     Train net output #2: loss = 2.4799 (* 1 = 2.4799 loss)
I0406 18:12:25.706893 86685 solver.cpp:578] Iteration 200, lr = 0.001
I0406 18:12:50.412609 86685 solver.cpp:247] Iteration 220, loss = 2.48341
I0406 18:12:50.426677 86685 solver.cpp:263]     Train net output #0: accuracy = 0.125
I0406 18:12:50.428061 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.375
I0406 18:12:50.431936 86685 solver.cpp:263]     Train net output #2: loss = 2.47607 (* 1 = 2.47607 loss)
I0406 18:12:50.432204 86685 solver.cpp:578] Iteration 220, lr = 0.001
I0406 18:13:14.964056 86685 solver.cpp:247] Iteration 240, loss = 2.48315
I0406 18:13:14.964123 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:13:14.964133 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.25
I0406 18:13:14.964155 86685 solver.cpp:263]     Train net output #2: loss = 2.47902 (* 1 = 2.47902 loss)
I0406 18:13:14.964165 86685 solver.cpp:578] Iteration 240, lr = 0.001
I0406 18:13:39.491210 86685 solver.cpp:247] Iteration 260, loss = 2.48281
I0406 18:13:39.491389 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:13:39.491423 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.4375
I0406 18:13:39.491459 86685 solver.cpp:263]     Train net output #2: loss = 2.47342 (* 1 = 2.47342 loss)
I0406 18:13:39.491468 86685 solver.cpp:578] Iteration 260, lr = 0.001
I0406 18:14:04.129600 86685 solver.cpp:247] Iteration 280, loss = 2.48238
I0406 18:14:04.129663 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:14:04.129675 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.1875
I0406 18:14:04.129688 86685 solver.cpp:263]     Train net output #2: loss = 2.47759 (* 1 = 2.47759 loss)
I0406 18:14:04.129704 86685 solver.cpp:578] Iteration 280, lr = 0.001
I0406 18:14:27.164858 86685 solver.cpp:360] Iteration 300, Testing net (#0)
I0406 18:14:32.671854 86685 solver.cpp:431]     Test net output #0: accuracy = 0.1375
I0406 18:14:32.671917 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.25
I0406 18:14:32.671931 86685 solver.cpp:431]     Test net output #2: loss = 2.47327 (* 1 = 2.47327 loss)
I0406 18:14:33.456264 86685 solver.cpp:247] Iteration 300, loss = 2.48198
I0406 18:14:33.456317 86685 solver.cpp:263]     Train net output #0: accuracy = 0
I0406 18:14:33.456328 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.125
I0406 18:14:33.456339 86685 solver.cpp:263]     Train net output #2: loss = 2.48143 (* 1 = 2.48143 loss)
I0406 18:14:33.456348 86685 solver.cpp:578] Iteration 300, lr = 0.001
I0406 18:14:57.932307 86685 solver.cpp:247] Iteration 320, loss = 2.48142
I0406 18:14:57.932504 86685 solver.cpp:263]     Train net output #0: accuracy = 0
I0406 18:14:57.932541 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.125
I0406 18:14:57.932566 86685 solver.cpp:263]     Train net output #2: loss = 2.48111 (* 1 = 2.48111 loss)
I0406 18:14:57.932577 86685 solver.cpp:578] Iteration 320, lr = 0.001
I0406 18:15:22.594286 86685 solver.cpp:247] Iteration 340, loss = 2.48081
I0406 18:15:22.602243 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:15:22.602257 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.25
I0406 18:15:22.602264 86685 solver.cpp:263]     Train net output #2: loss = 2.46998 (* 1 = 2.46998 loss)
I0406 18:15:22.602272 86685 solver.cpp:578] Iteration 340, lr = 0.001
I0406 18:15:47.119629 86685 solver.cpp:247] Iteration 360, loss = 2.4801
I0406 18:15:47.119812 86685 solver.cpp:263]     Train net output #0: accuracy = 0.25
I0406 18:15:47.119855 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.375
I0406 18:15:47.119885 86685 solver.cpp:263]     Train net output #2: loss = 2.46774 (* 1 = 2.46774 loss)
I0406 18:15:47.119915 86685 solver.cpp:578] Iteration 360, lr = 0.001
I0406 18:16:11.563837 86685 solver.cpp:247] Iteration 380, loss = 2.47909
I0406 18:16:11.563891 86685 solver.cpp:263]     Train net output #0: accuracy = 0.25
I0406 18:16:11.563901 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.4375
I0406 18:16:11.563911 86685 solver.cpp:263]     Train net output #2: loss = 2.45448 (* 1 = 2.45448 loss)
I0406 18:16:11.563920 86685 solver.cpp:578] Iteration 380, lr = 0.001
I0406 18:16:34.862175 86685 solver.cpp:360] Iteration 400, Testing net (#0)
I0406 18:16:40.391101 86685 solver.cpp:431]     Test net output #0: accuracy = 0.2125
I0406 18:16:40.391160 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.2875
I0406 18:16:40.391175 86685 solver.cpp:431]     Test net output #2: loss = 2.4546 (* 1 = 2.4546 loss)
I0406 18:16:41.231133 86685 solver.cpp:247] Iteration 400, loss = 2.47813
I0406 18:16:41.231187 86685 solver.cpp:263]     Train net output #0: accuracy = 0.125
I0406 18:16:41.231199 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.3125
I0406 18:16:41.231209 86685 solver.cpp:263]     Train net output #2: loss = 2.45644 (* 1 = 2.45644 loss)
I0406 18:16:41.231220 86685 solver.cpp:578] Iteration 400, lr = 0.001
I0406 18:17:05.725620 86685 solver.cpp:247] Iteration 420, loss = 2.47695
I0406 18:17:05.729194 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:17:05.729212 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.6875
I0406 18:17:05.729223 86685 solver.cpp:263]     Train net output #2: loss = 2.44853 (* 1 = 2.44853 loss)
I0406 18:17:05.729233 86685 solver.cpp:578] Iteration 420, lr = 0.001
I0406 18:17:30.265008 86685 solver.cpp:247] Iteration 440, loss = 2.4755
I0406 18:17:30.266875 86685 solver.cpp:263]     Train net output #0: accuracy = 0.3125
I0406 18:17:30.266886 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.4375
I0406 18:17:30.266897 86685 solver.cpp:263]     Train net output #2: loss = 2.4485 (* 1 = 2.4485 loss)
I0406 18:17:30.266908 86685 solver.cpp:578] Iteration 440, lr = 0.001
I0406 18:17:54.700021 86685 solver.cpp:247] Iteration 460, loss = 2.47378
I0406 18:17:54.700281 86685 solver.cpp:263]     Train net output #0: accuracy = 0.3125
I0406 18:17:54.700296 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.5
I0406 18:17:54.700327 86685 solver.cpp:263]     Train net output #2: loss = 2.44081 (* 1 = 2.44081 loss)
I0406 18:17:54.700341 86685 solver.cpp:578] Iteration 460, lr = 0.001
I0406 18:18:19.222681 86685 solver.cpp:247] Iteration 480, loss = 2.47159
I0406 18:18:19.222738 86685 solver.cpp:263]     Train net output #0: accuracy = 0.1875
I0406 18:18:19.222751 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.4375
I0406 18:18:19.222764 86685 solver.cpp:263]     Train net output #2: loss = 2.42376 (* 1 = 2.42376 loss)
I0406 18:18:19.222776 86685 solver.cpp:578] Iteration 480, lr = 0.001
I0406 18:18:42.498651 86685 solver.cpp:360] Iteration 500, Testing net (#0)
I0406 18:18:48.169734 86685 solver.cpp:431]     Test net output #0: accuracy = 0.2375
I0406 18:18:48.169790 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.4375
I0406 18:18:48.169806 86685 solver.cpp:431]     Test net output #2: loss = 2.38454 (* 1 = 2.38454 loss)
I0406 18:18:48.948639 86685 solver.cpp:247] Iteration 500, loss = 2.46879
I0406 18:18:48.948694 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:18:48.948705 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.5625
I0406 18:18:48.948716 86685 solver.cpp:263]     Train net output #2: loss = 2.39237 (* 1 = 2.39237 loss)
I0406 18:18:48.948726 86685 solver.cpp:578] Iteration 500, lr = 0.001
I0406 18:19:13.412236 86685 solver.cpp:247] Iteration 520, loss = 2.46506
I0406 18:19:13.412432 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:19:13.412458 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.625
I0406 18:19:13.412470 86685 solver.cpp:263]     Train net output #2: loss = 2.33518 (* 1 = 2.33518 loss)
I0406 18:19:13.412495 86685 solver.cpp:578] Iteration 520, lr = 0.001
I0406 18:19:37.964503 86685 solver.cpp:247] Iteration 540, loss = 2.46044
I0406 18:19:37.964563 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:19:37.964576 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.5
I0406 18:19:37.964587 86685 solver.cpp:263]     Train net output #2: loss = 2.32256 (* 1 = 2.32256 loss)
I0406 18:19:37.964598 86685 solver.cpp:578] Iteration 540, lr = 0.001
I0406 18:20:02.492290 86685 solver.cpp:247] Iteration 560, loss = 2.45451
I0406 18:20:02.492597 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:20:02.492629 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.625
I0406 18:20:02.492641 86685 solver.cpp:263]     Train net output #2: loss = 2.25873 (* 1 = 2.25873 loss)
I0406 18:20:02.492651 86685 solver.cpp:578] Iteration 560, lr = 0.001
I0406 18:20:27.084175 86685 solver.cpp:247] Iteration 580, loss = 2.44617
I0406 18:20:27.084228 86685 solver.cpp:263]     Train net output #0: accuracy = 0.4375
I0406 18:20:27.084239 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.625
I0406 18:20:27.084250 86685 solver.cpp:263]     Train net output #2: loss = 2.18663 (* 1 = 2.18663 loss)
I0406 18:20:27.084259 86685 solver.cpp:578] Iteration 580, lr = 0.001
I0406 18:20:50.349923 86685 solver.cpp:360] Iteration 600, Testing net (#0)
I0406 18:20:55.920640 86685 solver.cpp:431]     Test net output #0: accuracy = 0.1625
I0406 18:20:55.920703 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.5
I0406 18:20:55.920718 86685 solver.cpp:431]     Test net output #2: loss = 2.01268 (* 1 = 2.01268 loss)
I0406 18:20:56.805440 86685 solver.cpp:247] Iteration 600, loss = 2.43538
I0406 18:20:56.805491 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:20:56.805502 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.625
I0406 18:20:56.805513 86685 solver.cpp:263]     Train net output #2: loss = 2.01945 (* 1 = 2.01945 loss)
I0406 18:20:56.805528 86685 solver.cpp:578] Iteration 600, lr = 0.001
I0406 18:21:21.306967 86685 solver.cpp:247] Iteration 620, loss = 2.4235
I0406 18:21:21.307147 86685 solver.cpp:263]     Train net output #0: accuracy = 0.0625
I0406 18:21:21.307168 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.375
I0406 18:21:21.307206 86685 solver.cpp:263]     Train net output #2: loss = 2.09393 (* 1 = 2.09393 loss)
I0406 18:21:21.307216 86685 solver.cpp:578] Iteration 620, lr = 0.001
I0406 18:21:45.814604 86685 solver.cpp:247] Iteration 640, loss = 2.40895
I0406 18:21:45.814654 86685 solver.cpp:263]     Train net output #0: accuracy = 0.3125
I0406 18:21:45.814664 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.4375
I0406 18:21:45.814676 86685 solver.cpp:263]     Train net output #2: loss = 1.89941 (* 1 = 1.89941 loss)
I0406 18:21:45.814684 86685 solver.cpp:578] Iteration 640, lr = 0.001
I0406 18:22:10.381830 86685 solver.cpp:247] Iteration 660, loss = 2.39265
I0406 18:22:10.390450 86685 solver.cpp:263]     Train net output #0: accuracy = 0.25
I0406 18:22:10.390506 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.625
I0406 18:22:10.390519 86685 solver.cpp:263]     Train net output #2: loss = 1.82601 (* 1 = 1.82601 loss)
I0406 18:22:10.390555 86685 solver.cpp:578] Iteration 660, lr = 0.001
I0406 18:22:34.927384 86685 solver.cpp:247] Iteration 680, loss = 2.3742
I0406 18:22:34.927435 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:22:34.927446 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.75
I0406 18:22:34.927456 86685 solver.cpp:263]     Train net output #2: loss = 1.67583 (* 1 = 1.67583 loss)
I0406 18:22:34.927465 86685 solver.cpp:578] Iteration 680, lr = 0.001
I0406 18:22:58.311986 86685 solver.cpp:360] Iteration 700, Testing net (#0)
I0406 18:23:03.809582 86685 solver.cpp:431]     Test net output #0: accuracy = 0.4
I0406 18:23:03.809639 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.675
I0406 18:23:03.809654 86685 solver.cpp:431]     Test net output #2: loss = 1.55296 (* 1 = 1.55296 loss)
I0406 18:23:04.571346 86685 solver.cpp:247] Iteration 700, loss = 2.35434
I0406 18:23:04.571400 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:23:04.571413 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.5625
I0406 18:23:04.571424 86685 solver.cpp:263]     Train net output #2: loss = 1.66353 (* 1 = 1.66353 loss)
I0406 18:23:04.571432 86685 solver.cpp:578] Iteration 700, lr = 0.001
I0406 18:23:29.068984 86685 solver.cpp:247] Iteration 720, loss = 2.33354
I0406 18:23:29.071699 86685 solver.cpp:263]     Train net output #0: accuracy = 0.125
I0406 18:23:29.071717 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.5625
I0406 18:23:29.071729 86685 solver.cpp:263]     Train net output #2: loss = 1.66907 (* 1 = 1.66907 loss)
I0406 18:23:29.071740 86685 solver.cpp:578] Iteration 720, lr = 0.001
I0406 18:23:53.750185 86685 solver.cpp:247] Iteration 740, loss = 2.31264
I0406 18:23:53.750236 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:23:53.750247 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.5625
I0406 18:23:53.750259 86685 solver.cpp:263]     Train net output #2: loss = 1.6052 (* 1 = 1.6052 loss)
I0406 18:23:53.750268 86685 solver.cpp:578] Iteration 740, lr = 0.001
I0406 18:24:18.011466 86685 solver.cpp:247] Iteration 760, loss = 2.29007
I0406 18:24:18.017771 86685 solver.cpp:263]     Train net output #0: accuracy = 0.4375
I0406 18:24:18.017783 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.75
I0406 18:24:18.017792 86685 solver.cpp:263]     Train net output #2: loss = 1.40177 (* 1 = 1.40177 loss)
I0406 18:24:18.017802 86685 solver.cpp:578] Iteration 760, lr = 0.001
I0406 18:24:42.421557 86685 solver.cpp:247] Iteration 780, loss = 2.2668
I0406 18:24:42.421607 86685 solver.cpp:263]     Train net output #0: accuracy = 0.4375
I0406 18:24:42.421619 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.75
I0406 18:24:42.421632 86685 solver.cpp:263]     Train net output #2: loss = 1.41002 (* 1 = 1.41002 loss)
I0406 18:24:42.421643 86685 solver.cpp:578] Iteration 780, lr = 0.001
I0406 18:25:05.773908 86685 solver.cpp:360] Iteration 800, Testing net (#0)
I0406 18:25:11.176715 86685 solver.cpp:431]     Test net output #0: accuracy = 0.425
I0406 18:25:11.176777 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.8625
I0406 18:25:11.176792 86685 solver.cpp:431]     Test net output #2: loss = 1.25112 (* 1 = 1.25112 loss)
I0406 18:25:12.030035 86685 solver.cpp:247] Iteration 800, loss = 2.24477
I0406 18:25:12.030097 86685 solver.cpp:263]     Train net output #0: accuracy = 0.25
I0406 18:25:12.030108 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.75
I0406 18:25:12.030120 86685 solver.cpp:263]     Train net output #2: loss = 1.2851 (* 1 = 1.2851 loss)
I0406 18:25:12.030131 86685 solver.cpp:578] Iteration 800, lr = 0.001
I0406 18:25:36.585449 86685 solver.cpp:247] Iteration 820, loss = 2.22065
I0406 18:25:36.592125 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:25:36.592150 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:25:36.592159 86685 solver.cpp:263]     Train net output #2: loss = 1.25193 (* 1 = 1.25193 loss)
I0406 18:25:36.592169 86685 solver.cpp:578] Iteration 820, lr = 0.001
I0406 18:26:01.073673 86685 solver.cpp:247] Iteration 840, loss = 2.19556
I0406 18:26:01.073734 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:26:01.073748 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.875
I0406 18:26:01.073761 86685 solver.cpp:263]     Train net output #2: loss = 1.10159 (* 1 = 1.10159 loss)
I0406 18:26:01.073778 86685 solver.cpp:578] Iteration 840, lr = 0.001
I0406 18:26:25.521118 86685 solver.cpp:247] Iteration 860, loss = 2.17125
I0406 18:26:25.521338 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:26:25.521399 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:26:25.521411 86685 solver.cpp:263]     Train net output #2: loss = 1.08681 (* 1 = 1.08681 loss)
I0406 18:26:25.521420 86685 solver.cpp:578] Iteration 860, lr = 0.001
I0406 18:26:50.199426 86685 solver.cpp:247] Iteration 880, loss = 2.14614
I0406 18:26:50.199487 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:26:50.199499 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.8125
I0406 18:26:50.199512 86685 solver.cpp:263]     Train net output #2: loss = 1.02521 (* 1 = 1.02521 loss)
I0406 18:26:50.199520 86685 solver.cpp:578] Iteration 880, lr = 0.001
I0406 18:27:13.409546 86685 solver.cpp:360] Iteration 900, Testing net (#0)
I0406 18:27:19.107972 86685 solver.cpp:431]     Test net output #0: accuracy = 0.4375
I0406 18:27:19.108031 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.825
I0406 18:27:19.108043 86685 solver.cpp:431]     Test net output #2: loss = 1.18692 (* 1 = 1.18692 loss)
I0406 18:27:19.939749 86685 solver.cpp:247] Iteration 900, loss = 2.12034
I0406 18:27:19.939805 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5625
I0406 18:27:19.939816 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:27:19.939827 86685 solver.cpp:263]     Train net output #2: loss = 0.915096 (* 1 = 0.915096 loss)
I0406 18:27:19.939836 86685 solver.cpp:578] Iteration 900, lr = 0.001
I0406 18:27:44.391675 86685 solver.cpp:247] Iteration 920, loss = 2.09535
I0406 18:27:44.391923 86685 solver.cpp:263]     Train net output #0: accuracy = 0.3125
I0406 18:27:44.391937 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:27:44.391949 86685 solver.cpp:263]     Train net output #2: loss = 0.90459 (* 1 = 0.90459 loss)
I0406 18:27:44.391960 86685 solver.cpp:578] Iteration 920, lr = 0.001
I0406 18:28:08.853108 86685 solver.cpp:247] Iteration 940, loss = 2.07117
I0406 18:28:08.853163 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:28:08.853174 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:28:08.853185 86685 solver.cpp:263]     Train net output #2: loss = 0.888205 (* 1 = 0.888205 loss)
I0406 18:28:08.853194 86685 solver.cpp:578] Iteration 940, lr = 0.001
I0406 18:28:33.356480 86685 solver.cpp:247] Iteration 960, loss = 2.04817
I0406 18:28:33.356714 86685 solver.cpp:263]     Train net output #0: accuracy = 0.3125
I0406 18:28:33.356729 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:28:33.356740 86685 solver.cpp:263]     Train net output #2: loss = 0.964749 (* 1 = 0.964749 loss)
I0406 18:28:33.356755 86685 solver.cpp:578] Iteration 960, lr = 0.001
I0406 18:28:57.905457 86685 solver.cpp:247] Iteration 980, loss = 2.02346
I0406 18:28:57.905515 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:28:57.905527 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:28:57.905539 86685 solver.cpp:263]     Train net output #2: loss = 0.797788 (* 1 = 0.797788 loss)
I0406 18:28:57.905553 86685 solver.cpp:578] Iteration 980, lr = 0.001
I0406 18:29:21.864089 86685 solver.cpp:449] Snapshotting to snapshots_rgb_iter_1000.caffemodel
I0406 18:29:22.704558 86685 solver.cpp:458] Snapshotting solver state to snapshots_rgb_iter_1000.solverstate
I0406 18:29:23.162967 86685 solver.cpp:360] Iteration 1000, Testing net (#0)
I0406 18:29:28.269142 86685 solver.cpp:431]     Test net output #0: accuracy = 0.6375
I0406 18:29:28.269197 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.9
I0406 18:29:28.269212 86685 solver.cpp:431]     Test net output #2: loss = 0.990899 (* 1 = 0.990899 loss)
I0406 18:29:29.022907 86685 solver.cpp:247] Iteration 1000, loss = 1.99871
I0406 18:29:29.022955 86685 solver.cpp:263]     Train net output #0: accuracy = 0.4375
I0406 18:29:29.022966 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:29:29.022977 86685 solver.cpp:263]     Train net output #2: loss = 0.971661 (* 1 = 0.971661 loss)
I0406 18:29:29.022987 86685 solver.cpp:578] Iteration 1000, lr = 0.001
I0406 18:29:53.441969 86685 solver.cpp:247] Iteration 1020, loss = 1.96555
I0406 18:29:53.442250 86685 solver.cpp:263]     Train net output #0: accuracy = 0.4375
I0406 18:29:53.442268 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:29:53.442286 86685 solver.cpp:263]     Train net output #2: loss = 0.860018 (* 1 = 0.860018 loss)
I0406 18:29:53.442296 86685 solver.cpp:578] Iteration 1020, lr = 0.001
I0406 18:30:17.856122 86685 solver.cpp:247] Iteration 1040, loss = 1.93074
I0406 18:30:17.856181 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5625
I0406 18:30:17.856194 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:30:17.856204 86685 solver.cpp:263]     Train net output #2: loss = 0.731344 (* 1 = 0.731344 loss)
I0406 18:30:17.856215 86685 solver.cpp:578] Iteration 1040, lr = 0.001
I0406 18:30:42.120817 86685 solver.cpp:247] Iteration 1060, loss = 1.89574
I0406 18:30:42.121034 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:30:42.121060 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:30:42.121078 86685 solver.cpp:263]     Train net output #2: loss = 0.66223 (* 1 = 0.66223 loss)
I0406 18:30:42.121101 86685 solver.cpp:578] Iteration 1060, lr = 0.001
I0406 18:31:06.608927 86685 solver.cpp:247] Iteration 1080, loss = 1.86077
I0406 18:31:06.608989 86685 solver.cpp:263]     Train net output #0: accuracy = 0.375
I0406 18:31:06.609000 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:31:06.609012 86685 solver.cpp:263]     Train net output #2: loss = 0.88111 (* 1 = 0.88111 loss)
I0406 18:31:06.609024 86685 solver.cpp:578] Iteration 1080, lr = 0.001
I0406 18:31:29.863297 86685 solver.cpp:360] Iteration 1100, Testing net (#0)
I0406 18:31:35.327564 86685 solver.cpp:431]     Test net output #0: accuracy = 0.5375
I0406 18:31:35.327623 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.8875
I0406 18:31:35.327637 86685 solver.cpp:431]     Test net output #2: loss = 1.0549 (* 1 = 1.0549 loss)
I0406 18:31:36.140600 86685 solver.cpp:247] Iteration 1100, loss = 1.82501
I0406 18:31:36.140653 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5625
I0406 18:31:36.140663 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:31:36.140673 86685 solver.cpp:263]     Train net output #2: loss = 0.713442 (* 1 = 0.713442 loss)
I0406 18:31:36.140682 86685 solver.cpp:578] Iteration 1100, lr = 0.001
I0406 18:32:00.409567 86685 solver.cpp:247] Iteration 1120, loss = 1.78924
I0406 18:32:00.409862 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:32:00.409876 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:32:00.409888 86685 solver.cpp:263]     Train net output #2: loss = 0.587053 (* 1 = 0.587053 loss)
I0406 18:32:00.409903 86685 solver.cpp:578] Iteration 1120, lr = 0.001
I0406 18:32:24.747406 86685 solver.cpp:247] Iteration 1140, loss = 1.75296
I0406 18:32:24.752153 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:32:24.752187 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:32:24.752213 86685 solver.cpp:263]     Train net output #2: loss = 0.640455 (* 1 = 0.640455 loss)
I0406 18:32:24.752240 86685 solver.cpp:578] Iteration 1140, lr = 0.001
I0406 18:32:49.373894 86685 solver.cpp:247] Iteration 1160, loss = 1.71715
I0406 18:32:49.374127 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:32:49.374146 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:32:49.374171 86685 solver.cpp:263]     Train net output #2: loss = 0.69558 (* 1 = 0.69558 loss)
I0406 18:32:49.374197 86685 solver.cpp:578] Iteration 1160, lr = 0.001
I0406 18:33:13.795670 86685 solver.cpp:247] Iteration 1180, loss = 1.68076
I0406 18:33:13.795719 86685 solver.cpp:263]     Train net output #0: accuracy = 0.625
I0406 18:33:13.795732 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:33:13.795742 86685 solver.cpp:263]     Train net output #2: loss = 0.70835 (* 1 = 0.70835 loss)
I0406 18:33:13.795752 86685 solver.cpp:578] Iteration 1180, lr = 0.001
I0406 18:33:36.904228 86685 solver.cpp:360] Iteration 1200, Testing net (#0)
I0406 18:33:42.350008 86685 solver.cpp:431]     Test net output #0: accuracy = 0.5875
I0406 18:33:42.350065 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.95
I0406 18:33:42.350080 86685 solver.cpp:431]     Test net output #2: loss = 0.810034 (* 1 = 0.810034 loss)
I0406 18:33:43.195219 86685 solver.cpp:247] Iteration 1200, loss = 1.64409
I0406 18:33:43.195268 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:33:43.195278 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:33:43.195288 86685 solver.cpp:263]     Train net output #2: loss = 0.563655 (* 1 = 0.563655 loss)
I0406 18:33:43.195297 86685 solver.cpp:578] Iteration 1200, lr = 0.001
I0406 18:34:07.561705 86685 solver.cpp:247] Iteration 1220, loss = 1.60729
I0406 18:34:07.561924 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:34:07.561957 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:34:07.561992 86685 solver.cpp:263]     Train net output #2: loss = 0.71444 (* 1 = 0.71444 loss)
I0406 18:34:07.562028 86685 solver.cpp:578] Iteration 1220, lr = 0.001
I0406 18:34:32.083828 86685 solver.cpp:247] Iteration 1240, loss = 1.5707
I0406 18:34:32.083889 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:34:32.083900 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:34:32.083912 86685 solver.cpp:263]     Train net output #2: loss = 0.652364 (* 1 = 0.652364 loss)
I0406 18:34:32.083927 86685 solver.cpp:578] Iteration 1240, lr = 0.001
I0406 18:34:56.499244 86685 solver.cpp:247] Iteration 1260, loss = 1.53355
I0406 18:34:56.504209 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5
I0406 18:34:56.504230 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:34:56.504242 86685 solver.cpp:263]     Train net output #2: loss = 0.741566 (* 1 = 0.741566 loss)
I0406 18:34:56.504253 86685 solver.cpp:578] Iteration 1260, lr = 0.001
I0406 18:35:20.851898 86685 solver.cpp:247] Iteration 1280, loss = 1.49603
I0406 18:35:20.852016 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5625
I0406 18:35:20.852030 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:35:20.852041 86685 solver.cpp:263]     Train net output #2: loss = 0.628938 (* 1 = 0.628938 loss)
I0406 18:35:20.852057 86685 solver.cpp:578] Iteration 1280, lr = 0.001
I0406 18:35:43.999953 86685 solver.cpp:360] Iteration 1300, Testing net (#0)
I0406 18:35:49.686282 86685 solver.cpp:431]     Test net output #0: accuracy = 0.575
I0406 18:35:49.686337 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.95
I0406 18:35:49.686352 86685 solver.cpp:431]     Test net output #2: loss = 0.663297 (* 1 = 0.663297 loss)
I0406 18:35:50.490597 86685 solver.cpp:247] Iteration 1300, loss = 1.45891
I0406 18:35:50.490650 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:35:50.490661 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:35:50.490672 86685 solver.cpp:263]     Train net output #2: loss = 0.646157 (* 1 = 0.646157 loss)
I0406 18:35:50.490681 86685 solver.cpp:578] Iteration 1300, lr = 0.001
I0406 18:36:14.871383 86685 solver.cpp:247] Iteration 1320, loss = 1.4213
I0406 18:36:14.871675 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:36:14.871739 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:36:14.871773 86685 solver.cpp:263]     Train net output #2: loss = 0.460516 (* 1 = 0.460516 loss)
I0406 18:36:14.871803 86685 solver.cpp:578] Iteration 1320, lr = 0.001
I0406 18:36:39.270329 86685 solver.cpp:247] Iteration 1340, loss = 1.38387
I0406 18:36:39.270383 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:36:39.270395 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:36:39.270406 86685 solver.cpp:263]     Train net output #2: loss = 0.588288 (* 1 = 0.588288 loss)
I0406 18:36:39.270414 86685 solver.cpp:578] Iteration 1340, lr = 0.001
I0406 18:37:03.682740 86685 solver.cpp:247] Iteration 1360, loss = 1.34701
I0406 18:37:03.683019 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5625
I0406 18:37:03.683058 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:37:03.683074 86685 solver.cpp:263]     Train net output #2: loss = 0.694569 (* 1 = 0.694569 loss)
I0406 18:37:03.683084 86685 solver.cpp:578] Iteration 1360, lr = 0.001
I0406 18:37:28.051405 86685 solver.cpp:247] Iteration 1380, loss = 1.30941
I0406 18:37:28.051461 86685 solver.cpp:263]     Train net output #0: accuracy = 0.625
I0406 18:37:28.051473 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:37:28.051484 86685 solver.cpp:263]     Train net output #2: loss = 0.65453 (* 1 = 0.65453 loss)
I0406 18:37:28.051494 86685 solver.cpp:578] Iteration 1380, lr = 0.001
I0406 18:37:51.204495 86685 solver.cpp:360] Iteration 1400, Testing net (#0)
I0406 18:37:56.680721 86685 solver.cpp:431]     Test net output #0: accuracy = 0.6875
I0406 18:37:56.680784 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.9625
I0406 18:37:56.680799 86685 solver.cpp:431]     Test net output #2: loss = 0.599964 (* 1 = 0.599964 loss)
I0406 18:37:57.481489 86685 solver.cpp:247] Iteration 1400, loss = 1.27202
I0406 18:37:57.481542 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:37:57.481554 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:37:57.481564 86685 solver.cpp:263]     Train net output #2: loss = 0.60196 (* 1 = 0.60196 loss)
I0406 18:37:57.481575 86685 solver.cpp:578] Iteration 1400, lr = 0.001
I0406 18:38:21.855895 86685 solver.cpp:247] Iteration 1420, loss = 1.23511
I0406 18:38:21.856115 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:38:21.856156 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:38:21.856181 86685 solver.cpp:263]     Train net output #2: loss = 0.603869 (* 1 = 0.603869 loss)
I0406 18:38:21.856202 86685 solver.cpp:578] Iteration 1420, lr = 0.001
I0406 18:38:46.287664 86685 solver.cpp:247] Iteration 1440, loss = 1.19937
I0406 18:38:46.287719 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:38:46.287730 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:38:46.287741 86685 solver.cpp:263]     Train net output #2: loss = 0.516074 (* 1 = 0.516074 loss)
I0406 18:38:46.287750 86685 solver.cpp:578] Iteration 1440, lr = 0.001
I0406 18:39:10.673446 86685 solver.cpp:247] Iteration 1460, loss = 1.16254
I0406 18:39:10.673646 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:39:10.673666 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:39:10.673691 86685 solver.cpp:263]     Train net output #2: loss = 0.556967 (* 1 = 0.556967 loss)
I0406 18:39:10.673699 86685 solver.cpp:578] Iteration 1460, lr = 0.001
I0406 18:39:35.263509 86685 solver.cpp:247] Iteration 1480, loss = 1.12551
I0406 18:39:35.263562 86685 solver.cpp:263]     Train net output #0: accuracy = 0.625
I0406 18:39:35.263572 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:39:35.263582 86685 solver.cpp:263]     Train net output #2: loss = 0.575128 (* 1 = 0.575128 loss)
I0406 18:39:35.263592 86685 solver.cpp:578] Iteration 1480, lr = 0.001
I0406 18:39:58.440493 86685 solver.cpp:360] Iteration 1500, Testing net (#0)
I0406 18:40:03.954177 86685 solver.cpp:431]     Test net output #0: accuracy = 0.7375
I0406 18:40:03.954238 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.9875
I0406 18:40:03.954253 86685 solver.cpp:431]     Test net output #2: loss = 0.487135 (* 1 = 0.487135 loss)
I0406 18:40:04.706163 86685 solver.cpp:247] Iteration 1500, loss = 1.08877
I0406 18:40:04.706218 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:40:04.706228 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:40:04.706240 86685 solver.cpp:263]     Train net output #2: loss = 0.528273 (* 1 = 0.528273 loss)
I0406 18:40:04.706249 86685 solver.cpp:578] Iteration 1500, lr = 0.001
I0406 18:40:29.156427 86685 solver.cpp:247] Iteration 1520, loss = 1.05177
I0406 18:40:29.156669 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:40:29.156713 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:40:29.156738 86685 solver.cpp:263]     Train net output #2: loss = 0.592601 (* 1 = 0.592601 loss)
I0406 18:40:29.156746 86685 solver.cpp:578] Iteration 1520, lr = 0.001
I0406 18:40:53.627696 86685 solver.cpp:247] Iteration 1540, loss = 1.0152
I0406 18:40:53.627750 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:40:53.627761 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:40:53.627773 86685 solver.cpp:263]     Train net output #2: loss = 0.529055 (* 1 = 0.529055 loss)
I0406 18:40:53.627782 86685 solver.cpp:578] Iteration 1540, lr = 0.001
I0406 18:41:18.120571 86685 solver.cpp:247] Iteration 1560, loss = 0.979329
I0406 18:41:18.120782 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:41:18.120817 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:41:18.120846 86685 solver.cpp:263]     Train net output #2: loss = 0.483839 (* 1 = 0.483839 loss)
I0406 18:41:18.120870 86685 solver.cpp:578] Iteration 1560, lr = 0.001
I0406 18:41:42.573890 86685 solver.cpp:247] Iteration 1580, loss = 0.945619
I0406 18:41:42.573951 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:41:42.573961 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:41:42.573972 86685 solver.cpp:263]     Train net output #2: loss = 0.602244 (* 1 = 0.602244 loss)
I0406 18:41:42.573982 86685 solver.cpp:578] Iteration 1580, lr = 0.001
I0406 18:42:05.716265 86685 solver.cpp:360] Iteration 1600, Testing net (#0)
I0406 18:42:11.241158 86685 solver.cpp:431]     Test net output #0: accuracy = 0.65
I0406 18:42:11.241214 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.9625
I0406 18:42:11.241228 86685 solver.cpp:431]     Test net output #2: loss = 0.661717 (* 1 = 0.661717 loss)
I0406 18:42:12.056300 86685 solver.cpp:247] Iteration 1600, loss = 0.912921
I0406 18:42:12.056354 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:42:12.056365 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:42:12.056377 86685 solver.cpp:263]     Train net output #2: loss = 0.543462 (* 1 = 0.543462 loss)
I0406 18:42:12.056386 86685 solver.cpp:578] Iteration 1600, lr = 0.001
I0406 18:42:36.324851 86685 solver.cpp:247] Iteration 1620, loss = 0.881336
I0406 18:42:36.325057 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:42:36.325111 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:42:36.325139 86685 solver.cpp:263]     Train net output #2: loss = 0.385345 (* 1 = 0.385345 loss)
I0406 18:42:36.325150 86685 solver.cpp:578] Iteration 1620, lr = 0.001
I0406 18:43:00.906481 86685 solver.cpp:247] Iteration 1640, loss = 0.851154
I0406 18:43:00.906540 86685 solver.cpp:263]     Train net output #0: accuracy = 0.625
I0406 18:43:00.906550 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:43:00.906561 86685 solver.cpp:263]     Train net output #2: loss = 0.658199 (* 1 = 0.658199 loss)
I0406 18:43:00.906570 86685 solver.cpp:578] Iteration 1640, lr = 0.001
I0406 18:43:25.152657 86685 solver.cpp:247] Iteration 1660, loss = 0.823287
I0406 18:43:25.162051 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:43:25.162096 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:43:25.162122 86685 solver.cpp:263]     Train net output #2: loss = 0.412504 (* 1 = 0.412504 loss)
I0406 18:43:25.162142 86685 solver.cpp:578] Iteration 1660, lr = 0.001
I0406 18:43:49.426283 86685 solver.cpp:247] Iteration 1680, loss = 0.797095
I0406 18:43:49.426344 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:43:49.426355 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:43:49.426367 86685 solver.cpp:263]     Train net output #2: loss = 0.31474 (* 1 = 0.31474 loss)
I0406 18:43:49.426381 86685 solver.cpp:578] Iteration 1680, lr = 0.001
I0406 18:44:12.404340 86685 solver.cpp:360] Iteration 1700, Testing net (#0)
I0406 18:44:17.894350 86685 solver.cpp:431]     Test net output #0: accuracy = 0.675
I0406 18:44:17.894405 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.95
I0406 18:44:17.894421 86685 solver.cpp:431]     Test net output #2: loss = 0.794682 (* 1 = 0.794682 loss)
I0406 18:44:18.723047 86685 solver.cpp:247] Iteration 1700, loss = 0.772828
I0406 18:44:18.723098 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:44:18.723109 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:44:18.723119 86685 solver.cpp:263]     Train net output #2: loss = 0.295132 (* 1 = 0.295132 loss)
I0406 18:44:18.723129 86685 solver.cpp:578] Iteration 1700, lr = 0.001
I0406 18:44:43.126859 86685 solver.cpp:247] Iteration 1720, loss = 0.748973
I0406 18:44:43.127038 86685 solver.cpp:263]     Train net output #0: accuracy = 0.625
I0406 18:44:43.127053 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:44:43.127079 86685 solver.cpp:263]     Train net output #2: loss = 0.629946 (* 1 = 0.629946 loss)
I0406 18:44:43.127089 86685 solver.cpp:578] Iteration 1720, lr = 0.001
I0406 18:45:07.575199 86685 solver.cpp:247] Iteration 1740, loss = 0.726054
I0406 18:45:07.575255 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:45:07.575268 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:45:07.575278 86685 solver.cpp:263]     Train net output #2: loss = 0.495676 (* 1 = 0.495676 loss)
I0406 18:45:07.575289 86685 solver.cpp:578] Iteration 1740, lr = 0.001
I0406 18:45:31.956701 86685 solver.cpp:247] Iteration 1760, loss = 0.706005
I0406 18:45:31.956876 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:45:31.956898 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:45:31.956910 86685 solver.cpp:263]     Train net output #2: loss = 0.403201 (* 1 = 0.403201 loss)
I0406 18:45:31.956934 86685 solver.cpp:578] Iteration 1760, lr = 0.001
I0406 18:45:56.432569 86685 solver.cpp:247] Iteration 1780, loss = 0.68727
I0406 18:45:56.432615 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:45:56.432626 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:45:56.432636 86685 solver.cpp:263]     Train net output #2: loss = 0.522646 (* 1 = 0.522646 loss)
I0406 18:45:56.432646 86685 solver.cpp:578] Iteration 1780, lr = 0.001
I0406 18:46:19.669625 86685 solver.cpp:360] Iteration 1800, Testing net (#0)
I0406 18:46:25.217929 86685 solver.cpp:431]     Test net output #0: accuracy = 0.65
I0406 18:46:25.217985 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.9625
I0406 18:46:25.218000 86685 solver.cpp:431]     Test net output #2: loss = 0.543025 (* 1 = 0.543025 loss)
I0406 18:46:25.977517 86685 solver.cpp:247] Iteration 1800, loss = 0.668728
I0406 18:46:25.977571 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:46:25.977583 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:46:25.977596 86685 solver.cpp:263]     Train net output #2: loss = 0.491195 (* 1 = 0.491195 loss)
I0406 18:46:25.977605 86685 solver.cpp:578] Iteration 1800, lr = 0.001
I0406 18:46:50.335387 86685 solver.cpp:247] Iteration 1820, loss = 0.651796
I0406 18:46:50.335598 86685 solver.cpp:263]     Train net output #0: accuracy = 0.6875
I0406 18:46:50.335613 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:46:50.335650 86685 solver.cpp:263]     Train net output #2: loss = 0.476041 (* 1 = 0.476041 loss)
I0406 18:46:50.335672 86685 solver.cpp:578] Iteration 1820, lr = 0.001
I0406 18:47:14.673524 86685 solver.cpp:247] Iteration 1840, loss = 0.636738
I0406 18:47:14.673584 86685 solver.cpp:263]     Train net output #0: accuracy = 0.625
I0406 18:47:14.673594 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:47:14.673606 86685 solver.cpp:263]     Train net output #2: loss = 0.457464 (* 1 = 0.457464 loss)
I0406 18:47:14.673616 86685 solver.cpp:578] Iteration 1840, lr = 0.001
I0406 18:47:39.023270 86685 solver.cpp:247] Iteration 1860, loss = 0.621328
I0406 18:47:39.023540 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:47:39.023561 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:47:39.023573 86685 solver.cpp:263]     Train net output #2: loss = 0.289291 (* 1 = 0.289291 loss)
I0406 18:47:39.023583 86685 solver.cpp:578] Iteration 1860, lr = 0.001
I0406 18:48:03.370090 86685 solver.cpp:247] Iteration 1880, loss = 0.607859
I0406 18:48:03.370143 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:48:03.370154 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:48:03.370165 86685 solver.cpp:263]     Train net output #2: loss = 0.340068 (* 1 = 0.340068 loss)
I0406 18:48:03.370174 86685 solver.cpp:578] Iteration 1880, lr = 0.001
I0406 18:48:26.474969 86685 solver.cpp:360] Iteration 1900, Testing net (#0)
I0406 18:48:31.956053 86685 solver.cpp:431]     Test net output #0: accuracy = 0.6125
I0406 18:48:31.956112 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.85
I0406 18:48:31.956126 86685 solver.cpp:431]     Test net output #2: loss = 1.1612 (* 1 = 1.1612 loss)
I0406 18:48:32.745802 86685 solver.cpp:247] Iteration 1900, loss = 0.596706
I0406 18:48:32.745860 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:48:32.745870 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:48:32.745885 86685 solver.cpp:263]     Train net output #2: loss = 0.339248 (* 1 = 0.339248 loss)
I0406 18:48:32.745895 86685 solver.cpp:578] Iteration 1900, lr = 0.001
I0406 18:48:57.220897 86685 solver.cpp:247] Iteration 1920, loss = 0.585367
I0406 18:48:57.222537 86685 solver.cpp:263]     Train net output #0: accuracy = 1
I0406 18:48:57.222553 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:48:57.222576 86685 solver.cpp:263]     Train net output #2: loss = 0.183236 (* 1 = 0.183236 loss)
I0406 18:48:57.222586 86685 solver.cpp:578] Iteration 1920, lr = 0.001
I0406 18:49:21.664007 86685 solver.cpp:247] Iteration 1940, loss = 0.574131
I0406 18:49:21.664058 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:49:21.664069 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:49:21.664082 86685 solver.cpp:263]     Train net output #2: loss = 0.247146 (* 1 = 0.247146 loss)
I0406 18:49:21.664091 86685 solver.cpp:578] Iteration 1940, lr = 0.001
I0406 18:49:45.882105 86685 solver.cpp:247] Iteration 1960, loss = 0.56099
I0406 18:49:45.882350 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:49:45.882380 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:49:45.882390 86685 solver.cpp:263]     Train net output #2: loss = 0.392807 (* 1 = 0.392807 loss)
I0406 18:49:45.882400 86685 solver.cpp:578] Iteration 1960, lr = 0.001
I0406 18:50:10.163440 86685 solver.cpp:247] Iteration 1980, loss = 0.551173
I0406 18:50:10.163494 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:50:10.163506 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:50:10.163516 86685 solver.cpp:263]     Train net output #2: loss = 0.347933 (* 1 = 0.347933 loss)
I0406 18:50:10.163525 86685 solver.cpp:578] Iteration 1980, lr = 0.001
I0406 18:50:34.003311 86685 solver.cpp:449] Snapshotting to snapshots_rgb_iter_2000.caffemodel
I0406 18:50:34.750893 86685 solver.cpp:458] Snapshotting solver state to snapshots_rgb_iter_2000.solverstate
I0406 18:50:35.192627 86685 solver.cpp:360] Iteration 2000, Testing net (#0)
I0406 18:50:40.243993 86685 solver.cpp:431]     Test net output #0: accuracy = 0.6625
I0406 18:50:40.244050 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.925
I0406 18:50:40.244068 86685 solver.cpp:431]     Test net output #2: loss = 0.995041 (* 1 = 0.995041 loss)
I0406 18:50:41.002399 86685 solver.cpp:247] Iteration 2000, loss = 0.541998
I0406 18:50:41.002456 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:50:41.002468 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:50:41.002480 86685 solver.cpp:263]     Train net output #2: loss = 0.349828 (* 1 = 0.349828 loss)
I0406 18:50:41.002490 86685 solver.cpp:578] Iteration 2000, lr = 0.001
I0406 18:51:05.287801 86685 solver.cpp:247] Iteration 2020, loss = 0.533035
I0406 18:51:05.288110 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:51:05.288154 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:51:05.288166 86685 solver.cpp:263]     Train net output #2: loss = 0.428208 (* 1 = 0.428208 loss)
I0406 18:51:05.288177 86685 solver.cpp:578] Iteration 2020, lr = 0.001
I0406 18:51:29.711879 86685 solver.cpp:247] Iteration 2040, loss = 0.525336
I0406 18:51:29.711932 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:51:29.711943 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:51:29.711954 86685 solver.cpp:263]     Train net output #2: loss = 0.247023 (* 1 = 0.247023 loss)
I0406 18:51:29.711963 86685 solver.cpp:578] Iteration 2040, lr = 0.001
I0406 18:51:53.995575 86685 solver.cpp:247] Iteration 2060, loss = 0.51722
I0406 18:51:53.995811 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:51:53.995854 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:51:53.995867 86685 solver.cpp:263]     Train net output #2: loss = 0.583243 (* 1 = 0.583243 loss)
I0406 18:51:53.995887 86685 solver.cpp:578] Iteration 2060, lr = 0.001
I0406 18:52:18.328035 86685 solver.cpp:247] Iteration 2080, loss = 0.508599
I0406 18:52:18.328091 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:52:18.328102 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:52:18.328114 86685 solver.cpp:263]     Train net output #2: loss = 0.27798 (* 1 = 0.27798 loss)
I0406 18:52:18.328122 86685 solver.cpp:578] Iteration 2080, lr = 0.001
I0406 18:52:41.456821 86685 solver.cpp:360] Iteration 2100, Testing net (#0)
I0406 18:52:47.151337 86685 solver.cpp:431]     Test net output #0: accuracy = 0.7
I0406 18:52:47.151396 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.925
I0406 18:52:47.151410 86685 solver.cpp:431]     Test net output #2: loss = 0.775843 (* 1 = 0.775843 loss)
I0406 18:52:47.933588 86685 solver.cpp:247] Iteration 2100, loss = 0.501532
I0406 18:52:47.933640 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:52:47.933651 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:52:47.933662 86685 solver.cpp:263]     Train net output #2: loss = 0.248754 (* 1 = 0.248754 loss)
I0406 18:52:47.936310 86685 solver.cpp:578] Iteration 2100, lr = 0.001
I0406 18:53:12.296828 86685 solver.cpp:247] Iteration 2120, loss = 0.493044
I0406 18:53:12.297052 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:53:12.297093 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:53:12.297117 86685 solver.cpp:263]     Train net output #2: loss = 0.296345 (* 1 = 0.296345 loss)
I0406 18:53:12.297143 86685 solver.cpp:578] Iteration 2120, lr = 0.001
I0406 18:53:36.663977 86685 solver.cpp:247] Iteration 2140, loss = 0.486061
I0406 18:53:36.664034 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:53:36.664044 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:53:36.664055 86685 solver.cpp:263]     Train net output #2: loss = 0.406882 (* 1 = 0.406882 loss)
I0406 18:53:36.664064 86685 solver.cpp:578] Iteration 2140, lr = 0.001
I0406 18:54:01.029983 86685 solver.cpp:247] Iteration 2160, loss = 0.478907
I0406 18:54:01.037150 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:54:01.037173 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:54:01.037185 86685 solver.cpp:263]     Train net output #2: loss = 0.425759 (* 1 = 0.425759 loss)
I0406 18:54:01.037196 86685 solver.cpp:578] Iteration 2160, lr = 0.001
I0406 18:54:25.380584 86685 solver.cpp:247] Iteration 2180, loss = 0.470435
I0406 18:54:25.380638 86685 solver.cpp:263]     Train net output #0: accuracy = 0.75
I0406 18:54:25.380647 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:54:25.380657 86685 solver.cpp:263]     Train net output #2: loss = 0.449967 (* 1 = 0.449967 loss)
I0406 18:54:25.380666 86685 solver.cpp:578] Iteration 2180, lr = 0.001
I0406 18:54:48.434108 86685 solver.cpp:360] Iteration 2200, Testing net (#0)
I0406 18:54:53.858635 86685 solver.cpp:431]     Test net output #0: accuracy = 0.6875
I0406 18:54:53.858687 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.875
I0406 18:54:53.858701 86685 solver.cpp:431]     Test net output #2: loss = 1.04208 (* 1 = 1.04208 loss)
I0406 18:54:54.745748 86685 solver.cpp:247] Iteration 2200, loss = 0.463499
I0406 18:54:54.745803 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:54:54.745813 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:54:54.745833 86685 solver.cpp:263]     Train net output #2: loss = 0.512229 (* 1 = 0.512229 loss)
I0406 18:54:54.745856 86685 solver.cpp:578] Iteration 2200, lr = 0.001
I0406 18:55:19.154383 86685 solver.cpp:247] Iteration 2220, loss = 0.456042
I0406 18:55:19.154749 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:55:19.154778 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:55:19.154790 86685 solver.cpp:263]     Train net output #2: loss = 0.390784 (* 1 = 0.390784 loss)
I0406 18:55:19.154805 86685 solver.cpp:578] Iteration 2220, lr = 0.001
I0406 18:55:43.488469 86685 solver.cpp:247] Iteration 2240, loss = 0.44803
I0406 18:55:43.488523 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:55:43.488533 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:55:43.488543 86685 solver.cpp:263]     Train net output #2: loss = 0.217496 (* 1 = 0.217496 loss)
I0406 18:55:43.488553 86685 solver.cpp:578] Iteration 2240, lr = 0.001
I0406 18:56:07.908414 86685 solver.cpp:247] Iteration 2260, loss = 0.441045
I0406 18:56:07.908609 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:56:07.908638 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:56:07.908663 86685 solver.cpp:263]     Train net output #2: loss = 0.391168 (* 1 = 0.391168 loss)
I0406 18:56:07.908691 86685 solver.cpp:578] Iteration 2260, lr = 0.001
I0406 18:56:32.466548 86685 solver.cpp:247] Iteration 2280, loss = 0.434166
I0406 18:56:32.466603 86685 solver.cpp:263]     Train net output #0: accuracy = 0.8125
I0406 18:56:32.466614 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:56:32.466630 86685 solver.cpp:263]     Train net output #2: loss = 0.3246 (* 1 = 0.3246 loss)
I0406 18:56:32.466639 86685 solver.cpp:578] Iteration 2280, lr = 0.001
I0406 18:56:55.441627 86685 solver.cpp:360] Iteration 2300, Testing net (#0)
I0406 18:57:01.001194 86685 solver.cpp:431]     Test net output #0: accuracy = 0.65
I0406 18:57:01.001255 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.85
I0406 18:57:01.001269 86685 solver.cpp:431]     Test net output #2: loss = 1.11889 (* 1 = 1.11889 loss)
I0406 18:57:01.838696 86685 solver.cpp:247] Iteration 2300, loss = 0.425813
I0406 18:57:01.838749 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:57:01.838760 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:57:01.838771 86685 solver.cpp:263]     Train net output #2: loss = 0.267951 (* 1 = 0.267951 loss)
I0406 18:57:01.838780 86685 solver.cpp:578] Iteration 2300, lr = 0.001
I0406 18:57:26.038059 86685 solver.cpp:247] Iteration 2320, loss = 0.419104
I0406 18:57:26.038254 86685 solver.cpp:263]     Train net output #0: accuracy = 0.5625
I0406 18:57:26.038283 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 0.9375
I0406 18:57:26.038329 86685 solver.cpp:263]     Train net output #2: loss = 0.961091 (* 1 = 0.961091 loss)
I0406 18:57:26.038352 86685 solver.cpp:578] Iteration 2320, lr = 0.001
I0406 18:57:50.348016 86685 solver.cpp:247] Iteration 2340, loss = 0.415154
I0406 18:57:50.348069 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:57:50.348080 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:57:50.348091 86685 solver.cpp:263]     Train net output #2: loss = 0.340407 (* 1 = 0.340407 loss)
I0406 18:57:50.348100 86685 solver.cpp:578] Iteration 2340, lr = 0.001
I0406 18:58:14.606009 86685 solver.cpp:247] Iteration 2360, loss = 0.407717
I0406 18:58:14.606317 86685 solver.cpp:263]     Train net output #0: accuracy = 0.9375
I0406 18:58:14.606336 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:58:14.606349 86685 solver.cpp:263]     Train net output #2: loss = 0.122983 (* 1 = 0.122983 loss)
I0406 18:58:14.606359 86685 solver.cpp:578] Iteration 2360, lr = 0.001
I0406 18:58:38.972488 86685 solver.cpp:247] Iteration 2380, loss = 0.401063
I0406 18:58:38.972543 86685 solver.cpp:263]     Train net output #0: accuracy = 1
I0406 18:58:38.972553 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:58:38.972564 86685 solver.cpp:263]     Train net output #2: loss = 0.121428 (* 1 = 0.121428 loss)
I0406 18:58:38.972574 86685 solver.cpp:578] Iteration 2380, lr = 0.001
I0406 18:59:02.059370 86685 solver.cpp:360] Iteration 2400, Testing net (#0)
I0406 18:59:07.569991 86685 solver.cpp:431]     Test net output #0: accuracy = 0.675
I0406 18:59:07.570049 86685 solver.cpp:431]     Test net output #1: accuracy_top2 = 0.8625
I0406 18:59:07.570062 86685 solver.cpp:431]     Test net output #2: loss = 1.1251 (* 1 = 1.1251 loss)
I0406 18:59:08.395772 86685 solver.cpp:247] Iteration 2400, loss = 0.393566
I0406 18:59:08.395828 86685 solver.cpp:263]     Train net output #0: accuracy = 1
I0406 18:59:08.395838 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:59:08.395848 86685 solver.cpp:263]     Train net output #2: loss = 0.134288 (* 1 = 0.134288 loss)
I0406 18:59:08.395858 86685 solver.cpp:578] Iteration 2400, lr = 0.001
I0406 18:59:32.605321 86685 solver.cpp:247] Iteration 2420, loss = 0.38593
I0406 18:59:32.605538 86685 solver.cpp:263]     Train net output #0: accuracy = 1
I0406 18:59:32.605563 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:59:32.605581 86685 solver.cpp:263]     Train net output #2: loss = 0.112296 (* 1 = 0.112296 loss)
I0406 18:59:32.605605 86685 solver.cpp:578] Iteration 2420, lr = 0.001
I0406 18:59:56.901300 86685 solver.cpp:247] Iteration 2440, loss = 0.378404
I0406 18:59:56.901361 86685 solver.cpp:263]     Train net output #0: accuracy = 0.875
I0406 18:59:56.901372 86685 solver.cpp:263]     Train net output #1: accuracy_top2 = 1
I0406 18:59:56.901384 86685 solver.cpp:263]     Train net output #2: loss = 0.267895 (* 1 = 0.267895 loss)
I0406 18:59:56.901399 86685 solver.cpp:578] Iteration 2440, lr = 0.001
